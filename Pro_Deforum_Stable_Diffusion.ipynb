{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CVL1971/k-diffusion/blob/master/Pro_Deforum_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c442uQJ_gUgy"
      },
      "source": [
        "# **Pro Deforum Stable Diffusion v0.3**\n",
        "[Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer and the [Stability.ai](https://stability.ai/) Team. [K Diffusion](https://github.com/crowsonkb/k-diffusion) by [Katherine Crowson](https://twitter.com/RiversHaveWings). You need to get the ckpt file and put it on your Google Drive first to use this. It can be downloaded from [HuggingFace](https://huggingface.co/CompVis/stable-diffusion).\n",
        "\n",
        "Notebook by [deforum](https://discord.gg/upmXXsrwZc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4knibRpAQ06"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g-f7cQmf2Nt",
        "cellView": "form"
      },
      "source": [
        "#@markdown **NVIDIA GPU**\n",
        "import subprocess\n",
        "sub_p_res = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free', '--format=csv,noheader'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "print(sub_p_res)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TxIOPT0G5Lx1"
      },
      "source": [
        "#@markdown **Model and Output Paths**\n",
        "# ask for the link\n",
        "print(\"Local Path Variables:\\n\")\n",
        "\n",
        "models_path = \"/content/models\" #@param {type:\"string\"}\n",
        "output_path = \"/content/output\" #@param {type:\"string\"}\n",
        "temporal_path= \"/content/temporal\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Google Drive Path Variables (Optional)**\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "force_remount = False\n",
        "\n",
        "if mount_google_drive:\n",
        "    from google.colab import drive # type: ignore\n",
        "    try:\n",
        "        drive_path = \"/content/drive\"\n",
        "        drive.mount(drive_path,force_remount=force_remount)\n",
        "        models_path_gdrive = \"/content/drive/MyDrive/AI/models\" #@param {type:\"string\"}\n",
        "        output_path_gdrive = \"/content/drive/MyDrive/AI/StableDiffusion\" #@param {type:\"string\"}\n",
        "        models_path = models_path_gdrive\n",
        "        output_path = output_path_gdrive\n",
        "    except:\n",
        "        print(\"...error mounting drive or with drive path variables\")\n",
        "        print(\"...reverting to default path variables\")\n",
        "\n",
        "import os\n",
        "os.makedirs(models_path, exist_ok=True)\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "os.makedirs(temporal_path, exist_ok=True)\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")\n",
        "print(f\"temporal_path: {temporal_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRNl2mfepEIe",
        "cellView": "form"
      },
      "source": [
        "%%capture\n",
        "#@markdown **Setup Environment**\n",
        "\n",
        "setup_environment = True #@param {type:\"boolean\"}\n",
        "print_subprocess = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "if setup_environment:\n",
        "    import subprocess, time\n",
        "    print(\"Setting up environment...\")\n",
        "    start_time = time.time()\n",
        "    all_process = [\n",
        "        ['pip', 'install', 'pycryptodome'],\n",
        "        ['pip', 'install', 'torch==1.12.1+cu113', 'torchvision==0.13.1+cu113', '--extra-index-url', 'https://download.pytorch.org/whl/cu113'],\n",
        "        ['pip', 'install', 'omegaconf==2.2.3', 'einops==0.4.1', 'pytorch-lightning==1.7.4', 'torchmetrics==0.9.3', 'torchtext==0.13.1', 'transformers==4.21.2', 'kornia==0.6.7'],\n",
        "        #['git', 'clone', 'https://github.com/deforum/stable-diffusion'],\n",
        "        ['git', 'clone', 'https://github.com/CVL1971/stable-diffusion'],\n",
        "        ['pip', 'install', '-e', 'git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers'],\n",
        "        ['pip', 'install', '-e', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],\n",
        "        ['pip', 'install', 'accelerate', 'ftfy', 'jsonmerge', 'matplotlib', 'resize-right', 'timm', 'torchdiffeq'],\n",
        "        ['git', 'clone', 'https://github.com/shariqfarooq123/AdaBins.git'],\n",
        "        ['git', 'clone', 'https://github.com/isl-org/MiDaS.git'],\n",
        "        ['git', 'clone', 'https://github.com/MSFTserver/pytorch3d-lite.git'],\n",
        "    ]\n",
        "    for process in all_process:\n",
        "        running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if print_subprocess:\n",
        "            print(running)\n",
        "    \n",
        "    print(subprocess.run(['git', 'clone', 'https://github.com/CVL1971/k-diffusion/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    with open('k-diffusion/k_diffusion/__init__.py', 'w') as f:\n",
        "        f.write('')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Environment set up in {end_time-start_time:.0f} seconds\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81qmVZbrm4uu"
      },
      "source": [
        "#@markdown **Python Definitions**\n",
        "import json\n",
        "from IPython import display\n",
        "import math, os, pathlib, shutil, subprocess, sys, time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from skimage.exposure import match_histograms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from types import SimpleNamespace\n",
        "from torch import autocast\n",
        "\n",
        "sys.path.extend([\n",
        "    'src/taming-transformers',\n",
        "    'src/clip',\n",
        "    'stable-diffusion/',\n",
        "    'k-diffusion',\n",
        "    'pytorch3d-lite',\n",
        "    'AdaBins',\n",
        "    'MiDaS',\n",
        "])\n",
        "\n",
        "import py3d_tools as p3d\n",
        "from helpers import save_samples, sampler_fn\n",
        "from infer import InferenceHelper\n",
        "from k_diffusion import sampling\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "from midas.dpt_depth import DPTDepthModel\n",
        "from midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
        "\n",
        "#123\n",
        "#def tensor_to_image(tensor):\n",
        "#    tensor = tensor*255\n",
        "#    tensor = np.array(tensor, dtype=np.uint8)\n",
        "#    if np.ndim(tensor)>3:\n",
        "#        assert tensor.shape[0] == 1\n",
        "#        tensor = tensor[0]\n",
        "#    return PIL.Image.fromarray(tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sanitize(prompt):\n",
        "    whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "    tmp = ''.join(filter(whitelist.__contains__, prompt))\n",
        "    return '_'.join(prompt.split(\" \"))\n",
        "\n",
        "def anim_frame_warp_2d(prev_img_cv2, args, anim_args, keys, frame_idx):\n",
        "    angle = keys.angle_series[frame_idx]\n",
        "    zoom = keys.zoom_series[frame_idx]\n",
        "    translation_x = keys.translation_x_series[frame_idx]\n",
        "    translation_y = keys.translation_y_series[frame_idx]\n",
        "\n",
        "    center = (args.W // 2, args.H // 2)\n",
        "    trans_mat = np.float32([[1, 0, translation_x], [0, 1, translation_y]])\n",
        "    rot_mat = cv2.getRotationMatrix2D(center, angle, zoom)\n",
        "    trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "    rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "    xform = np.matmul(rot_mat, trans_mat)\n",
        "\n",
        "    return cv2.warpPerspective(\n",
        "        prev_img_cv2,\n",
        "        xform,\n",
        "        (prev_img_cv2.shape[1], prev_img_cv2.shape[0]),\n",
        "        borderMode=cv2.BORDER_WRAP if anim_args.border == 'wrap' else cv2.BORDER_REPLICATE\n",
        "    )\n",
        "\n",
        "def anim_frame_warp_3d(prev_img_cv2, anim_args, keys, frame_idx, adabins_helper, midas_model, midas_transform):\n",
        "    TRANSLATION_SCALE = 1.0/200.0 # matches Disco\n",
        "    translate_xyz = [\n",
        "        -keys.translation_x_series[frame_idx] * TRANSLATION_SCALE, \n",
        "        keys.translation_y_series[frame_idx] * TRANSLATION_SCALE, \n",
        "        -keys.translation_z_series[frame_idx] * TRANSLATION_SCALE\n",
        "    ]\n",
        "    rotate_xyz = [\n",
        "        math.radians(keys.rotation_3d_x_series[frame_idx]), \n",
        "        math.radians(keys.rotation_3d_y_series[frame_idx]), \n",
        "        math.radians(keys.rotation_3d_z_series[frame_idx])\n",
        "    ]\n",
        "    rot_mat = p3d.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
        "    result = transform_image_3d(prev_img_cv2, adabins_helper, midas_model, midas_transform, rot_mat, translate_xyz, anim_args)\n",
        "    torch.cuda.empty_cache()\n",
        "    return result\n",
        "\n",
        "def add_noise(sample: torch.Tensor, noise_amt: float):\n",
        "    return sample + torch.randn(sample.shape, device=sample.device) * noise_amt\n",
        "\n",
        "def download_depth_models():\n",
        "    def wget(url, outputdir):\n",
        "        print(subprocess.run(['wget', url, '-P', outputdir], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    if not os.path.exists(os.path.join(models_path, 'dpt_large-midas-2f21e586.pt')):\n",
        "        print(\"Downloading dpt_large-midas-2f21e586.pt...\")\n",
        "        wget(\"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\", models_path)\n",
        "    if not os.path.exists('pretrained/AdaBins_nyu.pt'):\n",
        "        print(\"Downloading AdaBins_nyu.pt...\")\n",
        "        os.makedirs('pretrained', exist_ok=True)\n",
        "        wget(\"https://cloudflare-ipfs.com/ipfs/Qmd2mMnDLWePKmgfS8m6ntAg4nhV5VkUyAydYBp8cWWeB7/AdaBins_nyu.pt\", 'pretrained')\n",
        "\n",
        "def get_output_folder(output_path, batch_folder):\n",
        "    out_path = os.path.join(output_path,time.strftime('%Y-%m'))\n",
        "    if batch_folder != \"\":\n",
        "        out_path = os.path.join(out_path, batch_folder)\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "    return out_path\n",
        "\n",
        "def load_depth_model(optimize=True):\n",
        "    midas_model = DPTDepthModel(\n",
        "        path=f\"{models_path}/dpt_large-midas-2f21e586.pt\",\n",
        "        backbone=\"vitl16_384\",\n",
        "        non_negative=True,\n",
        "    )\n",
        "    normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "\n",
        "    midas_transform = T.Compose([\n",
        "        Resize(\n",
        "            384, 384,\n",
        "            resize_target=None,\n",
        "            keep_aspect_ratio=True,\n",
        "            ensure_multiple_of=32,\n",
        "            resize_method=\"minimal\",\n",
        "            image_interpolation_method=cv2.INTER_CUBIC,\n",
        "        ),\n",
        "        normalization,\n",
        "        PrepareForNet()\n",
        "    ])\n",
        "\n",
        "    midas_model.eval()    \n",
        "    if optimize:\n",
        "        if device == torch.device(\"cuda\"):\n",
        "            midas_model = midas_model.to(memory_format=torch.channels_last)\n",
        "            midas_model = midas_model.half()\n",
        "    midas_model.to(device)\n",
        "\n",
        "    return midas_model, midas_transform\n",
        "\n",
        "def load_img(path, shape):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "    image = image.resize(shape, resample=Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def load_mask_img(path, shape):\n",
        "    # path (str): Path to the mask image\n",
        "    # shape (list-like len(4)): shape of the image to match, usually latent_image.shape\n",
        "    mask_w_h = (shape[-1], shape[-2])\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        mask_image = Image.open(requests.get(path, stream=True).raw).convert('RGBA')\n",
        "    else:\n",
        "        mask_image = Image.open(path).convert('RGBA')\n",
        "    mask = mask_image.resize(mask_w_h, resample=Image.LANCZOS)\n",
        "    mask = mask.convert(\"L\")\n",
        "    return mask\n",
        "\n",
        "def maintain_colors(prev_img, color_match_sample, mode):\n",
        "    if mode == 'Match Frame 0 RGB':\n",
        "        return match_histograms(prev_img, color_match_sample, multichannel=True)\n",
        "    elif mode == 'Match Frame 0 HSV':\n",
        "        prev_img_hsv = cv2.cvtColor(prev_img, cv2.COLOR_RGB2HSV)\n",
        "        color_match_hsv = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2HSV)\n",
        "        matched_hsv = match_histograms(prev_img_hsv, color_match_hsv, multichannel=True)\n",
        "        return cv2.cvtColor(matched_hsv, cv2.COLOR_HSV2RGB)\n",
        "    else: # Match Frame 0 LAB\n",
        "        prev_img_lab = cv2.cvtColor(prev_img, cv2.COLOR_RGB2LAB)\n",
        "        color_match_lab = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2LAB)\n",
        "        matched_lab = match_histograms(prev_img_lab, color_match_lab, multichannel=True)\n",
        "        return cv2.cvtColor(matched_lab, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "\n",
        "def make_callback(sampler_name, dynamic_threshold=None, static_threshold=None, mask=None, init_latent=None, sigmas=None, sampler=None, masked_noise_modifier=1.0, showdiffusion=None, savediffusion=None, listsaveddiffusion=None, skipsamples=None, countsamples=None):  \n",
        "    display.display('make_callback')\n",
        "    # Creates the callback function to be passed into the samplers\n",
        "    # The callback function is applied to the image at each step\n",
        "    def dynamic_thresholding_(img, threshold):\n",
        "        #123\n",
        "        display.display('dynamic_thresholding_')\n",
        "        # Dynamic thresholding from Imagen paper (May 2022)\n",
        "        s = np.percentile(np.abs(img.cpu()), threshold, axis=tuple(range(1,img.ndim)))\n",
        "        s = np.max(np.append(s,1.0))\n",
        "        torch.clamp_(img, -1*s, s)\n",
        "        torch.FloatTensor.div_(img, s)\n",
        "      \n",
        "      \n",
        "\n",
        "    # Callback for samplers in the k-diffusion repo, called thus:\n",
        "    #   callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "    def k_callback_(args_dict):\n",
        "        #123\n",
        "        #456\n",
        "        \n",
        "        \n",
        "  \n",
        "       \n",
        "        #display.display(vimage)\n",
        "        # vfilename=time.strftime('%Y-%m-%H-%M-%S') + '.PNG'\n",
        "        # vfileoutput=os.path.join('/content/drive/MyDrive/AI/StableDiffusion',vfilename)\n",
        "        # vimage.save(vfileoutput)\n",
        "        # tfile=open(vfileoutput, \"rb\")\n",
        "        # wimgresult.value=tfile.read()\n",
        "\n",
        "        brest=(countsamples.value() % skipsamples)\n",
        "        if ((showdiffusion or savediffusion) and (brest==0)):\n",
        "          v1=model.decode_first_stage(args_dict['denoised'])\n",
        "          imgin8=sample_to_cv2(v1)\n",
        "          vimage=Image.fromarray(imgin8)\n",
        "\n",
        "          if (showdiffusion):\n",
        "              #print (countsamples.value())\n",
        "              buf = io.BytesIO()\n",
        "              vimage.save(buf, format='PNG')\n",
        "              wimgresult.value=buf.getvalue()\n",
        "              buf.flush\n",
        "              buf.close\n",
        "             \n",
        "              if savediffusion:\n",
        "                listsaveddiffusion.append(vimage)\n",
        "              else:\n",
        "                vimage.close\n",
        "            \n",
        "        countsamples.inc()\n",
        "\n",
        "        if dynamic_threshold is not None:\n",
        "            dynamic_thresholding_(args_dict['x'], dynamic_threshold)\n",
        "            print(\"dynamic\")\n",
        "        if static_threshold is not None:\n",
        "            torch.clamp_(args_dict['x'], -1*static_threshold, static_threshold)\n",
        "            print(\"static\")\n",
        "        if mask is not None:\n",
        "            init_noise = init_latent + noise * args_dict['sigma']\n",
        "            is_masked = torch.logical_and(mask >= mask_schedule[args_dict['i']], mask != 0 )\n",
        "            new_img = init_noise * torch.where(is_masked,1,0) + args_dict['x'] * torch.where(is_masked,0,1)\n",
        "            args_dict['x'].copy_(new_img)\n",
        "\n",
        "\n",
        "\n",
        "    # Function that is called on the image (img) and step (i) at each step\n",
        "    def img_callback_(img, i):\n",
        "        #123\n",
        "        display.display('img_callback_')\n",
        "        #print(f\"1\")\n",
        "        #img2=tensor_to_image(img.cpu())\n",
        "        #img2.save(\"hello\")\n",
        "        # Thresholding functions\n",
        "        \n",
        "        if dynamic_threshold is not None:\n",
        "            dynamic_thresholding_(img, dynamic_threshold)\n",
        "        if static_threshold is not None:\n",
        "            torch.clamp_(img, -1*static_threshold, static_threshold)\n",
        "        if mask is not None:\n",
        "            i_inv = len(sigmas) - i - 1\n",
        "            init_noise = sampler.stochastic_encode(init_latent, torch.tensor([i_inv]*batch_size).to(device), noise=noise)\n",
        "            is_masked = torch.logical_and(mask >= mask_schedule[i], mask != 0 )\n",
        "            new_img = init_noise * torch.where(is_masked,1,0) + img * torch.where(is_masked,0,1)\n",
        "            img.copy_(new_img)\n",
        "                   \n",
        "    if init_latent is not None:\n",
        "        noise = torch.randn_like(init_latent, device=device) * masked_noise_modifier\n",
        "    if sigmas is not None and len(sigmas) > 0:\n",
        "        mask_schedule, _ = torch.sort(sigmas/torch.max(sigmas))\n",
        "    elif len(sigmas) == 0:\n",
        "        mask = None # no mask needed if no steps (usually happens because strength==1.0)\n",
        "    if sampler_name in [\"plms\",\"ddim\"]: \n",
        "        # Callback function formated for compvis latent diffusion samplers\n",
        "        if mask is not None:\n",
        "            assert sampler is not None, \"Callback function for stable-diffusion samplers requires sampler variable\"\n",
        "            batch_size = init_latent.shape[0]\n",
        "\n",
        "        callback = img_callback_\n",
        "    else: \n",
        "        # Default callback function uses k-diffusion sampler variables\n",
        "        callback = k_callback_\n",
        "\n",
        "    return callback\n",
        "\n",
        "def prepare_mask(mask_file, mask_shape, mask_brightness_adjust=1.0, mask_contrast_adjust=1.0):\n",
        "    # path (str): Path to the mask image\n",
        "    # shape (list-like len(4)): shape of the image to match, usually latent_image.shape\n",
        "    # mask_brightness_adjust (non-negative float): amount to adjust brightness of the iamge, \n",
        "    #     0 is black, 1 is no adjustment, >1 is brighter\n",
        "    # mask_contrast_adjust (non-negative float): amount to adjust contrast of the image, \n",
        "    #     0 is a flat grey image, 1 is no adjustment, >1 is more contrast\n",
        "                            \n",
        "    mask = load_mask_img(mask_file, mask_shape)\n",
        "\n",
        "    # Mask brightness/contrast adjustments\n",
        "    if mask_brightness_adjust != 1:\n",
        "        mask = TF.adjust_brightness(mask, mask_brightness_adjust)\n",
        "    if mask_contrast_adjust != 1:\n",
        "        mask = TF.adjust_contrast(mask, mask_contrast_adjust)\n",
        "\n",
        "    # Mask image to array\n",
        "    mask = np.array(mask).astype(np.float32) / 255.0\n",
        "    mask = np.tile(mask,(4,1,1))\n",
        "    mask = np.expand_dims(mask,axis=0)\n",
        "    mask = torch.from_numpy(mask)\n",
        "\n",
        "    if args.invert_mask:\n",
        "        mask = ( (mask - 0.5) * -1) + 0.5\n",
        "    \n",
        "    mask = np.clip(mask,0,1)\n",
        "    return mask\n",
        "\n",
        "def sample_from_cv2(sample: np.ndarray) -> torch.Tensor:\n",
        "    sample = ((sample.astype(float) / 255.0) * 2) - 1\n",
        "    sample = sample[None].transpose(0, 3, 1, 2).astype(np.float16)\n",
        "    sample = torch.from_numpy(sample)\n",
        "    return sample\n",
        "\n",
        "def sample_to_cv2(sample: torch.Tensor) -> np.ndarray:\n",
        "    sample_f32 = rearrange(sample.squeeze().cpu().numpy(), \"c h w -> h w c\").astype(np.float32)\n",
        "    sample_f32 = ((sample_f32 * 0.5) + 0.5).clip(0, 1)\n",
        "    sample_int8 = (sample_f32 * 255).astype(np.uint8)\n",
        "    return sample_int8\n",
        "\n",
        "@torch.no_grad()\n",
        "def transform_image_3d(prev_img_cv2, adabins_helper, midas_model, midas_transform, rot_mat, translate, anim_args):\n",
        "    # adapted and optimized version of transform_image_3d from Disco Diffusion https://github.com/alembics/disco-diffusion \n",
        "\n",
        "    w, h = prev_img_cv2.shape[1], prev_img_cv2.shape[0]\n",
        "\n",
        "    # predict depth with AdaBins    \n",
        "    use_adabins = anim_args.midas_weight < 1.0 and adabins_helper is not None\n",
        "    if use_adabins:\n",
        "        print(f\"Estimating depth of {w}x{h} image with AdaBins...\")\n",
        "        MAX_ADABINS_AREA = 500000\n",
        "        MIN_ADABINS_AREA = 448*448\n",
        "\n",
        "        # resize image if too large or too small\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(prev_img_cv2, cv2.COLOR_RGB2BGR))\n",
        "        image_pil_area = w*h\n",
        "        resized = True\n",
        "        if image_pil_area > MAX_ADABINS_AREA:\n",
        "            scale = math.sqrt(MAX_ADABINS_AREA) / math.sqrt(image_pil_area)\n",
        "            depth_input = img_pil.resize((int(w*scale), int(h*scale)), Image.LANCZOS) # LANCZOS is good for downsampling\n",
        "            print(f\"  resized to {depth_input.width}x{depth_input.height}\")\n",
        "        elif image_pil_area < MIN_ADABINS_AREA:\n",
        "            scale = math.sqrt(MIN_ADABINS_AREA) / math.sqrt(image_pil_area)\n",
        "            depth_input = img_pil.resize((int(w*scale), int(h*scale)), Image.BICUBIC)\n",
        "            print(f\"  resized to {depth_input.width}x{depth_input.height}\")\n",
        "        else:\n",
        "            depth_input = img_pil\n",
        "            resized = False\n",
        "\n",
        "        # predict depth and resize back to original dimensions\n",
        "        try:\n",
        "            _, adabins_depth = adabins_helper.predict_pil(depth_input)\n",
        "            if resized:\n",
        "                adabins_depth = torchvision.transforms.functional.resize(\n",
        "                    torch.from_numpy(adabins_depth), \n",
        "                    torch.Size([h, w]),\n",
        "                    interpolation=torchvision.transforms.functional.InterpolationMode.BICUBIC\n",
        "                )\n",
        "            adabins_depth = adabins_depth.squeeze()\n",
        "        except:\n",
        "            print(f\"  exception encountered, falling back to pure MiDaS\")\n",
        "            use_adabins = False\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if midas_model is not None:\n",
        "        # convert image from 0->255 uint8 to 0->1 float for feeding to MiDaS\n",
        "        img_midas = prev_img_cv2.astype(np.float32) / 255.0\n",
        "        img_midas_input = midas_transform({\"image\": img_midas})[\"image\"]\n",
        "\n",
        "        # MiDaS depth estimation implementation\n",
        "        print(f\"Estimating depth of {w}x{h} image with MiDaS...\")\n",
        "        sample = torch.from_numpy(img_midas_input).float().to(device).unsqueeze(0)\n",
        "        if device == torch.device(\"cuda\"):\n",
        "            sample = sample.to(memory_format=torch.channels_last)  \n",
        "            sample = sample.half()\n",
        "        midas_depth = midas_model.forward(sample)\n",
        "        midas_depth = torch.nn.functional.interpolate(\n",
        "            midas_depth.unsqueeze(1),\n",
        "            size=img_midas.shape[:2],\n",
        "            mode=\"bicubic\",\n",
        "            align_corners=False,\n",
        "        ).squeeze()\n",
        "        midas_depth = midas_depth.cpu().numpy()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # MiDaS makes the near values greater, and the far values lesser. Let's reverse that and try to align with AdaBins a bit better.\n",
        "        midas_depth = np.subtract(50.0, midas_depth)\n",
        "        midas_depth = midas_depth / 19.0\n",
        "\n",
        "        # blend between MiDaS and AdaBins predictions\n",
        "        if use_adabins:\n",
        "            depth_map = midas_depth*anim_args.midas_weight + adabins_depth*(1.0-anim_args.midas_weight)\n",
        "        else:\n",
        "            depth_map = midas_depth\n",
        "\n",
        "        depth_map = np.expand_dims(depth_map, axis=0)\n",
        "        depth_tensor = torch.from_numpy(depth_map).squeeze().to(device)\n",
        "    else:\n",
        "        depth_tensor = torch.ones((h, w), device=device)\n",
        "\n",
        "    pixel_aspect = 1.0 # aspect of an individual pixel (so usually 1.0)\n",
        "    near, far, fov_deg = anim_args.near_plane, anim_args.far_plane, anim_args.fov\n",
        "    persp_cam_old = p3d.FoVPerspectiveCameras(near, far, pixel_aspect, fov=fov_deg, degrees=True, device=device)\n",
        "    persp_cam_new = p3d.FoVPerspectiveCameras(near, far, pixel_aspect, fov=fov_deg, degrees=True, R=rot_mat, T=torch.tensor([translate]), device=device)\n",
        "\n",
        "    # range of [-1,1] is important to torch grid_sample's padding handling\n",
        "    y,x = torch.meshgrid(torch.linspace(-1.,1.,h,dtype=torch.float32,device=device),torch.linspace(-1.,1.,w,dtype=torch.float32,device=device))\n",
        "    z = torch.as_tensor(depth_tensor, dtype=torch.float32, device=device)\n",
        "    xyz_old_world = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
        "\n",
        "    xyz_old_cam_xy = persp_cam_old.get_full_projection_transform().transform_points(xyz_old_world)[:,0:2]\n",
        "    xyz_new_cam_xy = persp_cam_new.get_full_projection_transform().transform_points(xyz_old_world)[:,0:2]\n",
        "\n",
        "    offset_xy = xyz_new_cam_xy - xyz_old_cam_xy\n",
        "    # affine_grid theta param expects a batch of 2D mats. Each is 2x3 to do rotation+translation.\n",
        "    identity_2d_batch = torch.tensor([[1.,0.,0.],[0.,1.,0.]], device=device).unsqueeze(0)\n",
        "    # coords_2d will have shape (N,H,W,2).. which is also what grid_sample needs.\n",
        "    coords_2d = torch.nn.functional.affine_grid(identity_2d_batch, [1,1,h,w], align_corners=False)\n",
        "    offset_coords_2d = coords_2d - torch.reshape(offset_xy, (h,w,2)).unsqueeze(0)\n",
        "\n",
        "    image_tensor = torchvision.transforms.functional.to_tensor(Image.fromarray(prev_img_cv2)).to(device)\n",
        "    new_image = torch.nn.functional.grid_sample(\n",
        "        image_tensor.add(1/512 - 0.0001).unsqueeze(0), \n",
        "        offset_coords_2d, \n",
        "        mode=anim_args.sampling_mode, \n",
        "        padding_mode=anim_args.padding_mode, \n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    # convert back to cv2 style numpy array 0->255 uint8\n",
        "    result = rearrange(\n",
        "        new_image.squeeze().clamp(0,1) * 255.0, \n",
        "        'c h w -> h w c'\n",
        "    ).cpu().numpy().astype(np.uint8)\n",
        "    return result\n",
        "\n",
        "def generate(args, return_latent=False, return_sample=False, return_c=False):\n",
        "    seed_everything(args.seed)\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "\n",
        "    if args.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = CompVisDenoiser(model)       \n",
        "    batch_size = args.n_samples\n",
        "    prompt = args.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    init_latent = None\n",
        "    if args.init_latent is not None:\n",
        "        init_latent = args.init_latent\n",
        "    elif args.init_sample is not None:\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(args.init_sample))\n",
        "  \n",
        "    elif args.use_init and args.init_image != None and args.init_image != '':\n",
        "        init_image = load_img(args.init_image, shape=(args.W, args.H)).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space        \n",
        "\n",
        "    if not args.use_init and args.strength > 0:\n",
        "        print(\"\\nNo init image, but strength > 0. This may give you some strange results.\\n\")\n",
        "\n",
        "    # Mask functions\n",
        "    mask = None\n",
        "    if args.use_mask:\n",
        "        assert args.mask_file is not None, \"use_mask==True: An mask image is required for a mask\"\n",
        "        assert args.use_init, \"use_mask==True: use_init is required for a mask\"\n",
        "        assert init_latent is not None, \"use_mask==True: An latent init image is required for a mask\"\n",
        "\n",
        "        mask = prepare_mask(args.mask_file, \n",
        "                            init_latent.shape, \n",
        "                            args.mask_contrast_adjust, \n",
        "                            args.mask_brightness_adjust)\n",
        "        \n",
        "        mask = mask.to(device)\n",
        "        mask = repeat(mask, '1 ... -> b ...', b=batch_size)\n",
        "        \n",
        "    t_enc = int((1.0-args.strength) * args.steps)\n",
        "\n",
        "    # Noise schedule for the k-diffusion samplers (used for masking)\n",
        "    k_sigmas = model_wrap.get_sigmas(args.steps)\n",
        "    k_sigmas = k_sigmas[len(k_sigmas)-t_enc-1:]\n",
        "\n",
        "    if args.sampler in ['plms','ddim']:\n",
        "        sampler.make_schedule(ddim_num_steps=args.steps, ddim_eta=args.ddim_eta, ddim_discretize='fill', verbose=False)\n",
        "\n",
        "    callback = make_callback(sampler_name=args.sampler,\n",
        "                            dynamic_threshold=args.dynamic_threshold, \n",
        "                            static_threshold=args.static_threshold,\n",
        "                            mask=mask, \n",
        "                            init_latent=init_latent,\n",
        "                            sigmas=k_sigmas,\n",
        "                            sampler=sampler,\n",
        "                            showdiffusion=args.showinterimg,\n",
        "                            savediffusion=args.saveinterimg,\n",
        "                            listsaveddiffusion=args.listinterimg,\n",
        "                            skipsamples=args.skipsamples,\n",
        "                            countsamples=args.countsamples)    \n",
        "\n",
        "    results = []\n",
        "    precision_scope = autocast if args.precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for prompts in data:\n",
        "                    uc = None\n",
        "                    if args.scale != 1.0:\n",
        "                        #uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        uc = model.get_learned_conditioning([args.negativeprompt])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "                    c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                    if args.init_c != None:\n",
        "                        c = args.init_c\n",
        "\n",
        "                    if args.sampler in [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"]:\n",
        "                        samples = sampler_fn(\n",
        "                            c=c, \n",
        "                            uc=uc, \n",
        "                            args=args, \n",
        "                            model_wrap=model_wrap, \n",
        "                            init_latent=init_latent, \n",
        "                            t_enc=t_enc, \n",
        "                            device=device, \n",
        "                            cb=callback)\n",
        "                    else:\n",
        "                        # args.sampler == 'plms' or args.sampler == 'ddim':\n",
        "                        if init_latent is not None and args.strength > 0:\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                        else:\n",
        "                            z_enc = torch.randn([args.n_samples, args.C, args.H // args.f, args.W // args.f], device=device)\n",
        "                        if args.sampler == 'ddim':\n",
        "                            samples = sampler.decode(z_enc, \n",
        "                                                     c, \n",
        "                                                     t_enc, \n",
        "                                                     unconditional_guidance_scale=args.scale,\n",
        "                                                     unconditional_conditioning=uc,\n",
        "                                                     img_callback=callback)\n",
        "                        elif args.sampler == 'plms': # no \"decode\" function in plms, so use \"sample\"\n",
        "                            shape = [args.C, args.H // args.f, args.W // args.f]\n",
        "                            samples, _ = sampler.sample(S=args.steps,\n",
        "                                                            conditioning=c,\n",
        "                                                            batch_size=args.n_samples,\n",
        "                                                            shape=shape,\n",
        "                                                            verbose=False,\n",
        "                                                            unconditional_guidance_scale=args.scale,\n",
        "                                                            unconditional_conditioning=uc,\n",
        "                                                            eta=args.ddim_eta,\n",
        "                                                            x_T=z_enc,\n",
        "                                                            img_callback=callback)\n",
        "                        else:\n",
        "                            raise Exception(f\"Sampler {args.sampler} not recognised.\")\n",
        "\n",
        "                    if return_latent:\n",
        "                        results.append(samples.clone())\n",
        "\n",
        "                    x_samples = model.decode_first_stage(samples)\n",
        "                    if return_sample:\n",
        "                        results.append(x_samples.clone())\n",
        "\n",
        "                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                    if return_c:\n",
        "                        results.append(c.clone())\n",
        "\n",
        "                    for x_sample in x_samples:\n",
        "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                        image = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                        results.append(image)\n",
        "    return results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIUJ7lWI4v53",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Select and Load Model**\n",
        "\n",
        "model_config = \"v1-inference.yaml\" #@param [\"custom\",\"v1-inference.yaml\"]\n",
        "model_checkpoint =  \"sd-v1-4.ckpt\" #@param [\"custom\",\"sd-v1-4-full-ema.ckpt\",\"sd-v1-4.ckpt\",\"sd-v1-3-full-ema.ckpt\",\"sd-v1-3.ckpt\",\"sd-v1-2-full-ema.ckpt\",\"sd-v1-2.ckpt\",\"sd-v1-1-full-ema.ckpt\",\"sd-v1-1.ckpt\",\"robo-diffusion-v1.ckpt\"]\n",
        "custom_config_path = \"\" #@param {type:\"string\"}\n",
        "custom_checkpoint_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "load_on_run_all = True #@param {type: 'boolean'}\n",
        "half_precision = True # check\n",
        "check_sha256 = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_map = {\n",
        "    \"sd-v1-4-full-ema.ckpt\": {'sha256': '14749efc0ae8ef0329391ad4436feb781b402f4fece4883c7ad8d10556d8a36a'},\n",
        "    \"sd-v1-4.ckpt\": {'sha256': 'fe4efff1e174c627256e44ec2991ba279b3816e364b49f9be2abc0b3ff3f8556'},\n",
        "    \"sd-v1-3-full-ema.ckpt\": {'sha256': '54632c6e8a36eecae65e36cb0595fab314e1a1545a65209f24fde221a8d4b2ca'},\n",
        "    \"sd-v1-3.ckpt\": {'sha256': '2cff93af4dcc07c3e03110205988ff98481e86539c51a8098d4f2236e41f7f2f'},\n",
        "    \"sd-v1-2-full-ema.ckpt\": {'sha256': 'bc5086a904d7b9d13d2a7bccf38f089824755be7261c7399d92e555e1e9ac69a'},\n",
        "    \"sd-v1-2.ckpt\": {'sha256': '3b87d30facd5bafca1cbed71cfb86648aad75d1c264663c0cc78c7aea8daec0d'},\n",
        "    \"sd-v1-1-full-ema.ckpt\": {'sha256': 'efdeb5dc418a025d9a8cc0a8617e106c69044bc2925abecc8a254b2910d69829'},\n",
        "    \"sd-v1-1.ckpt\": {'sha256': '86cd1d3ccb044d7ba8db743d717c9bac603c4043508ad2571383f954390f3cea'},\n",
        "    \"robo-diffusion-v1.ckpt\": {'sha256': '244dbe0dcb55c761bde9c2ac0e9b46cc9705ebfe5f1f3a7cc46251573ea14e16'}\n",
        "\n",
        "# ================================================================================================ #\n",
        "# NOTE: for robo-diffusion to work you must have the words 'nousr' and 'robot' next to each other  #\n",
        "# ================================================================================================ #\n",
        "# Example: 'A photo of a nousr robot'                                                              #\n",
        "# Example: 'A water color painting of a nousr robot, in the style of studio ghibli'                #\n",
        "# ================================================================================================ #\n",
        "\n",
        "#prompts = [\n",
        "#    \"A water color painting of a nousr robot, in the style of studio ghibli\",\n",
        "#]\n",
        "\n",
        "    \n",
        "}\n",
        "\n",
        "# config path\n",
        "ckpt_config_path = custom_config_path if model_config == \"custom\" else os.path.join(models_path, model_config)\n",
        "if os.path.exists(ckpt_config_path):\n",
        "    print(f\"{ckpt_config_path} exists\")\n",
        "else:\n",
        "    ckpt_config_path = \"./stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n",
        "print(f\"Using config: {ckpt_config_path}\")\n",
        "\n",
        "# checkpoint path or download\n",
        "ckpt_path = custom_checkpoint_path if model_checkpoint == \"custom\" else os.path.join(models_path, model_checkpoint)\n",
        "ckpt_valid = True\n",
        "if os.path.exists(ckpt_path):\n",
        "    print(f\"{ckpt_path} exists\")\n",
        "else:\n",
        "    print(f\"Please download model checkpoint and place in {os.path.join(models_path, model_checkpoint)}\")\n",
        "    ckpt_valid = False\n",
        "\n",
        "if check_sha256 and model_checkpoint != \"custom\" and ckpt_valid:\n",
        "    import hashlib\n",
        "    print(\"\\n...checking sha256\")\n",
        "    with open(ckpt_path, \"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest()\n",
        "        del bytes\n",
        "    if model_map[model_checkpoint][\"sha256\"] == hash:\n",
        "        print(\"hash is correct\\n\")\n",
        "    else:\n",
        "        print(\"hash in not correct\\n\")\n",
        "        ckpt_valid = False\n",
        "       \n",
        "if ckpt_valid:\n",
        "    print(f\"Using ckpt: {ckpt_path}\")\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False, device='cuda', half_precision=True):\n",
        "    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=map_location)\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    if half_precision:\n",
        "        model = model.half().to(device)\n",
        "    else:\n",
        "        model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "if load_on_run_all and ckpt_valid:\n",
        "    local_config = OmegaConf.load(f\"{ckpt_config_path}\")\n",
        "    model = load_model_from_config(local_config, f\"{ckpt_path}\",half_precision=half_precision)\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model = model.to(device)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@markdown **Auxiliar Instalations**\n",
        "Execute=True #@param {type:\"boolean\"}\n",
        "if Execute:\n",
        "  # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "  !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "  %cd Real-ESRGAN\n",
        "  # Set up the environment\n",
        "  !pip install basicsr\n",
        "  !pip install facexlib\n",
        "  !pip install gfpgan\n",
        "  !pip install -r requirements.txt\n",
        "  !python setup.py develop\n",
        "  #warning import duplicate?\n",
        "  import matplotlib.pyplot as plt\n",
        "  def display2(img1, img2):\n",
        "    fig = plt.figure(figsize=(25, 10))\n",
        "    ax1 = fig.add_subplot(1, 2, 1) \n",
        "    plt.title('Input', fontsize=16)\n",
        "    ax1.axis('off')\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "    plt.title('CodeFormer', fontsize=16)\n",
        "    ax2.axis('off')\n",
        "    ax1.imshow(img1)\n",
        "    ax2.imshow(img2)\n",
        "  def imread(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "  #################################\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kFreILgAkEW4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Widgets imports\n",
        "# Imports for JupyterLite\n",
        "try:\n",
        "    import piplite\n",
        "    await piplite.install(['ipywidgets'])\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nasgRhJgvH4b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown **Auxiliar functions**\n",
        "\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "from os import remove\n",
        "\n",
        "try:\n",
        "  from base64 import b64encode\n",
        "  from base64 import b64decode\n",
        "  from IPython.display import HTML\n",
        "  from google.colab.output import eval_js\n",
        "  from Crypto.Cipher import AES\n",
        "  from Crypto.Util.Padding import pad\n",
        "  from Crypto.Util.Padding import unpad\n",
        "  from google.colab.output import eval_js\n",
        "except:\n",
        "  print('import error')\n",
        "\n",
        "\n",
        "\n",
        "def encript(data):\n",
        "  iv=   b'\\xaf\\x00\\n\\x96\\xf5\\x9f\\xde\\x02\\xb8 \\x88w\\xf2\\x83\\x99\\x9e'\n",
        "  key = b'&\\xbf\\x18\\x1f\\x0b\\x12\\x06\\x0c\\x0e\\xc6m\\xd6\\xaf0uZ\\xfdb\\x89\\xe7o\\xd8\\xff=D\\xde\\x86H\\x03j\\xc2,'\n",
        "  data = bytes(data, 'utf-8')\n",
        "  cipher = AES.new(key, AES.MODE_CBC, iv)\n",
        "\n",
        "  ct_bytes = cipher.encrypt(pad(data, AES.block_size))\n",
        "  iv = b64encode(iv).decode('utf-8')\n",
        "  ct = b64encode(ct_bytes).decode('utf-8')\n",
        "  return ct\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "def decript(data):\n",
        "  iv=   b'\\xaf\\x00\\n\\x96\\xf5\\x9f\\xde\\x02\\xb8 \\x88w\\xf2\\x83\\x99\\x9e'\n",
        "  key = b'&\\xbf\\x18\\x1f\\x0b\\x12\\x06\\x0c\\x0e\\xc6m\\xd6\\xaf0uZ\\xfdb\\x89\\xe7o\\xd8\\xff=D\\xde\\x86H\\x03j\\xc2,'\n",
        "  try:\n",
        "      ct = b64decode(data)\n",
        "      cipher = AES.new(key, AES.MODE_CBC, iv)\n",
        "      pt = unpad(cipher.decrypt(ct), AES.block_size)\n",
        "      return pt.decode(\"utf-8\")\n",
        "  except: \n",
        "      print(\"Incorrect decryption\")\n",
        "\n",
        "def cbool(sbool):\n",
        "  if sbool == \"True\" or sbool==\"true\": return True \n",
        "  elif sbool==\"False\" or sbool==\"false\": return False\n",
        "  else: return \"null\"\n",
        "\n",
        "\n",
        "def ParamToMeta(filename, args):\n",
        "    filepath=args.outdir\n",
        "    OriginFilePath=os.path.join(filepath,filename + \".png\")\n",
        "    TargetFilePath=os.path.join(filepath, filename + \"_meta.png\")\n",
        "    targetImage = Image.open(OriginFilePath)\n",
        "    metadata = PngInfo()\n",
        "    metadata.add_text(\"v\",'0.1')\n",
        "    metadata.add_text(\"enc\",'1')\n",
        "    metadata.add_text(\"W\", str(args.W))\n",
        "    metadata.add_text(\"H\", str(args.H))\n",
        "    metadata.add_text(\"sampler\", args.sampler)\n",
        "    metadata.add_text(\"steps\", str(args.steps))\n",
        "    metadata.add_text(\"scale\", str(args.scale))\n",
        "    metadata.add_text(\"ddim_eta\", str(args.ddim_eta))\n",
        "    metadata.add_text(\"dynamic_threshold\", \"null\")\n",
        "    metadata.add_text(\"static_threshold\", \"null\")\n",
        "    metadata.add_text(\"save_samples\", str(args.save_samples))\n",
        "    metadata.add_text(\"save_settings\", str(args.save_settings))\n",
        "    metadata.add_text(\"display_samples\", str(args.display_samples))\n",
        "    metadata.add_text(\"n_batch\", str(args.n_batch))\n",
        "    metadata.add_text(\"batch_name\", args.batch_name)\n",
        "    metadata.add_text(\"seed_behavior\", args.seed_behavior)\n",
        "    metadata.add_text(\"make_grid\", str(args.make_grid))\n",
        "    metadata.add_text(\"grid_rows\", str(args.grid_rows))\n",
        "    metadata.add_text(\"outdir\", args.outdir)\n",
        "    metadata.add_text(\"use_init\", str(args.use_init))\n",
        "    metadata.add_text(\"strength\", str(args.strength))\n",
        "    metadata.add_text(\"use_mask\", str(args.use_mask))\n",
        "    metadata.add_text(\"mask_file\", args.mask_file)\n",
        "    metadata.add_text(\"invert_mask\", str(args.invert_mask))\n",
        "    metadata.add_text(\"mask_brightness_adjust\", str(args.mask_brightness_adjust))\n",
        "    metadata.add_text(\"mask_contrast_adjust\", str(args.mask_contrast_adjust))\n",
        "    metadata.add_text(\"n_samples\", str(args.n_samples))\n",
        "    metadata.add_text(\"precision\", args.precision)\n",
        "    metadata.add_text(\"C\", str(args.C))\n",
        "    metadata.add_text(\"f\", str(args.f))\n",
        "    metadata.add_text(\"timestring\", args.timestring)\n",
        "    metadata.add_text(\"init_latent\", \"null\")\n",
        "    metadata.add_text(\"init_sample\", \"null\")\n",
        "    metadata.add_text(\"init_c\", \"null\")\n",
        "    metadata.add_text(\"prompts\", \"\")\n",
        "    metadata.add_text(\"negativeprompt\",encript(args.negativeprompt))\n",
        "    metadata.add_text(\"seed\", encript(str(args.seed)))\n",
        "    metadata.add_text(\"filename_format\", encript(args.filename_format))\n",
        "    metadata.add_text(\"init_image\", encript(args.init_image))\n",
        "    metadata.add_text(\"prompt\", encript(args.prompt))\n",
        "    targetImage.save(TargetFilePath, pnginfo=metadata)\n",
        "    print(targetImage.info)\n",
        "    targetImage.close()\n",
        "    remove(OriginFilePath)\n",
        "    return TargetFilePath\n",
        "\n",
        "\n",
        "def MetaToParam(vdict, args):\n",
        "  try:\n",
        "    vdict.get('v')\n",
        "    args.ww.value= int(vdict.get('W'))\n",
        "    args.wh.value= np.int_(vdict.get('H'))\n",
        "    args.wsampler.value= (vdict.get('sampler'))\n",
        "    args.wsteps.value= int(vdict.get('steps'))\n",
        "    args.wscale.value= float(vdict.get('scale'))\n",
        "    args.wddim_eta.value= float(vdict.get('ddim_eta'))\n",
        "    args.wsave_samples.value= cbool(vdict.get('save_samples'))\n",
        "    args.wdisplay_samples.value= cbool(vdict.get('display_samples'))\n",
        "    #args.wn_batch.value= int(vdict.get('n_batch'))\n",
        "    args.wn_batch.value= 1\n",
        "    args.wbatch_name.value= (vdict.get('batch_name'))\n",
        "    args.wseed_behavior.value= (vdict.get('seed_behavior'))\n",
        "    args.wmake_grid.value= cbool(vdict.get('make_grid'))\n",
        "    args.wgrid_rows.value= int(vdict.get('grid_rows'))\n",
        "    args.wuse_init.value= cbool(vdict.get('use_init'))\n",
        "    args.wstrength.value= float(vdict.get('strength'))\n",
        "    args.wuse_mask.value= cbool(vdict.get('use_mask'))\n",
        "    args.wmask_file.value= (vdict.get('mask_file'))\n",
        "    args.winvert_mask.value= cbool(vdict.get('invert_mask'))\n",
        "    args.wmask_brightness_adjust.value= float(vdict.get('mask_brightness_adjust'))\n",
        "    args.wmask_contrast_adjust.value= float(vdict.get('mask_contrast_adjust'))\n",
        "    args.wprompt.value= decript(vdict.get('prompt'))\n",
        "    args.wseed.value= int(decript(vdict.get('seed')))\n",
        "    args.wnegativeprompt.value=decript(vdict.get('negativeprompt'))\n",
        "    args.wfilename_format.value= decript(vdict.get('filename_format'))\n",
        "    args.winit_image.value= decript(vdict.get('init_image'))\n",
        "  except:\n",
        "    try:\n",
        "      args.ww.value= int(vdict.get('W'))\n",
        "      args.wh.value= np.int_(vdict.get('H'))\n",
        "      args.wsampler.value= (vdict.get('sampler'))\n",
        "      args.wsteps.value= int(vdict.get('steps'))\n",
        "      args.wscale.value= float(vdict.get('scale'))\n",
        "      args.wddim_eta.value= float(vdict.get('ddim_eta'))\n",
        "      args.wsave_samples.value= cbool(vdict.get('save_samples'))\n",
        "      args.wdisplay_samples.value= cbool(vdict.get('display_samples'))\n",
        "      #args.wn_batch.value= int(vdict.get('n_batch'))\n",
        "      args.wn_batch.value= 1\n",
        "      args.wbatch_name.value= (vdict.get('batch_name'))\n",
        "      args.wseed_behavior.value= (vdict.get('seed_behavior'))\n",
        "      args.wmake_grid.value= cbool(vdict.get('make_grid'))\n",
        "      args.wgrid_rows.value= int(vdict.get('grid_rows'))\n",
        "      args.wuse_init.value= cbool(vdict.get('use_init'))\n",
        "      args.wstrength.value= float(vdict.get('strength'))\n",
        "      args.wuse_mask.value= cbool(vdict.get('use_mask'))\n",
        "      args.wmask_file.value= (vdict.get('mask_file'))\n",
        "      args.winvert_mask.value= cbool(vdict.get('invert_mask'))\n",
        "      args.wmask_brightness_adjust.value= float(vdict.get('mask_brightness_adjust'))\n",
        "      args.wmask_contrast_adjust.value= float(vdict.get('mask_contrast_adjust'))\n",
        "      args.wprompt.value= (vdict.get('prompt'))\n",
        "      args.wseed.value= int(vdict.get('seed'))\n",
        "      args.wnegativeprompt.value=(vdict.get('negativeprompt'))\n",
        "      args.wfilename_format.value= (vdict.get('filename_format'))\n",
        "      args.winit_image.value= (vdict.get('init_image'))\n",
        "    except:\n",
        "      print(\"Metadata Not found, parameteres as default\")\n",
        "\n",
        "canvas_html = \"\"\"\n",
        "  \n",
        "<img src=%s id=\"imgsource\" width=\"512\" height=\"512\" hidden></img>\n",
        "\n",
        "\n",
        "<canvas id=\"canvas1\" width=%d height=%d style=\"border: 1px solid black\"></canvas>\n",
        "<canvas id=\"canvas2\" width=%d height=%d style=\"border: 1px solid black\" hidden></canvas>\n",
        "<button type=\"button\" id=\"exit\">Click Me!</button>\n",
        "\n",
        "<script>\n",
        "\n",
        "var canvas = document.getElementById('canvas1')\n",
        "var canvas2= document.getElementById('canvas2')\n",
        "var chica= document.getElementById('imgsource')\n",
        "var ctx = canvas.getContext('2d')\n",
        "var ctx2= canvas2.getContext('2d')\n",
        "ctx2.fillStyle = 'white';\n",
        "ctx2.fillRect( 0, 0, canvas2.width, canvas2.height)\n",
        "ctx.lineWidth = %d\n",
        "ctx2.lineWidth = ctx.lineWidth\n",
        "ctx.drawImage(chica, 0, 0, 512, 512);\n",
        "\n",
        "\n",
        "\n",
        "var button = document.getElementById(\"exit\")\n",
        "var mouse = {x: 0, y: 0}\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "\n",
        "  ctx2.beginPath()\n",
        "  ctx2.moveTo(mouse.x, mouse.y)\n",
        "  \n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "\n",
        "  ctx2.lineTo(mouse.x, mouse.y)\n",
        "  ctx2.stroke()\n",
        "}\n",
        "var data = new Promise(resolve=>{\n",
        " button.onclick = ()=>{\n",
        "    canvas.parentNode.removeChild(canvas);\n",
        "    chica.parentNode.removeChild(chica)\n",
        "    button.parentNode.removeChild(button)\n",
        "    resolve(canvas2.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def buildcanvas(w, h, line_width, imgin):\n",
        " \n",
        " png64=pngtobase64(imgin)\n",
        " return  canvas_html % (png64, w, h, w, h, line_width)\n",
        "  \n",
        "\n",
        "\n",
        "def cpaint(htmlcanvas, imageout):\n",
        "  display.display(HTML(htmlcanvas))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(imageout, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return len(binary) \n",
        "\n",
        "def pngtobase64(filepath):\n",
        "    encoded = (base64.b64encode(open(filepath, \"rb\").read())).decode('ascii')\n",
        "    return 'data:image/png;base64,{}'.format(encoded)\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "xig0jyRsEUMB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown **Param Functions**\n",
        "\n",
        "\n",
        "\n",
        "def InitParams():\n",
        "\n",
        " \n",
        "  wretro=False\n",
        "  wretro=widgets.Label(value=\"False\")\n",
        "  #####---ddim_eta----\n",
        "\n",
        "  wddim_eta=widgets.FloatSlider(\n",
        "      value=0.75,\n",
        "      min=0,\n",
        "      max=1,\n",
        "      step=0.05,\n",
        "      description='ddim_eta',\n",
        "      disabled=False,\n",
        "      continuous_update=False,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='.2f',\n",
        "  )\n",
        "\n",
        "  display.display(wddim_eta)\n",
        "\n",
        "  #####---save_samples----\n",
        "\n",
        "  wsave_samples=widgets.Checkbox(\n",
        "      value=True,\n",
        "      description='Save Samples',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        "  display.display(wsave_samples)\n",
        "\n",
        "  #####---save_settings----\n",
        "\n",
        "  wsave_settings=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Save Settings',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        "  display.display(wsave_settings)\n",
        "\n",
        "  #####---display_samples----\n",
        "\n",
        "  wdisplay_samples=widgets.Checkbox(\n",
        "      value=True,\n",
        "      description='display_samples',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        "  display.display(wdisplay_samples)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  wfilename_format=widgets.Dropdown(\n",
        "      options=[\"{timestring}_{index}_{seed}.png\",\"{timestring}_{index}_{prompt}.png\",\"{timestring}_{index}_{steps}_{seed}.png\"],\n",
        "      value='{timestring}_{index}_{seed}.png',\n",
        "      description='filename format',\n",
        "      disabled=False,\n",
        "  )\n",
        "\n",
        "\n",
        "  display.display(wfilename_format)\n",
        "\n",
        "  #####---seed_behavior----\n",
        "\n",
        "  wseed_behavior=widgets.Dropdown(\n",
        "      options=[\"iter\",\"fixed\",\"random\"],\n",
        "      value='iter',\n",
        "      description='seed behavior',\n",
        "      disabled=False,\n",
        "  )\n",
        "\n",
        "  display.display(wseed_behavior)\n",
        "\n",
        "  #####---make_grid----\n",
        "\n",
        "  wmake_grid=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='make grid',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        "  display.display(wmake_grid)\n",
        "\n",
        "  #####---grid_rows----\n",
        "\n",
        "  wgrid_rows=widgets.IntText(\n",
        "      value=2,\n",
        "      description='grid rows',\n",
        "      disabled=False\n",
        "  )\n",
        "\n",
        "  display.display(wgrid_rows)\n",
        "\n",
        "  #####---use_init----\n",
        "\n",
        "  wuse_init=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='use init',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        "  display.display(wuse_init)\n",
        "\n",
        "  \n",
        "\n",
        "  #####---init_image----\n",
        "\n",
        "  winit_image=widgets.Text(\n",
        "      value='',\n",
        "      placeholder='Path from Image to ImgtoImg',\n",
        "      description='init image',\n",
        "      disabled=False   \n",
        "  )\n",
        "\n",
        "  display.display(winit_image)\n",
        "\n",
        "  #####---use_mask----\n",
        "\n",
        "  wuse_mask=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='use mask',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        "  display.display(wuse_mask)\n",
        "\n",
        "  #####---mask_file----\n",
        "\n",
        "  wmask_file=widgets.Text(\n",
        "      value='',\n",
        "      placeholder='Path from mask to ImgtoImg',\n",
        "      description='mask file',\n",
        "      disabled=False   \n",
        "  )\n",
        "\n",
        "  display.display(wmask_file)\n",
        "\n",
        "  #####---invert_mask----\n",
        "\n",
        "  winvert_mask=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='invert mask',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        "  display.display(winvert_mask)\n",
        "\n",
        "  #####---mask_brightness_adjust----\n",
        "\n",
        "  wmask_brightness_adjust=widgets.FloatSlider(\n",
        "      value=1,\n",
        "      min=0,\n",
        "      max=1,\n",
        "      step=0.05,\n",
        "      description='mask brightness',\n",
        "      disabled=False,\n",
        "      continuous_update=False,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='.2f',\n",
        "  )\n",
        "\n",
        "  display.display(wmask_brightness_adjust)\n",
        "\n",
        "  #####---mask_contrast_adjust----\n",
        "\n",
        "  wmask_contrast_adjust=widgets.FloatSlider(\n",
        "      value=1,\n",
        "      min=0,\n",
        "      max=1,\n",
        "      step=0.05,\n",
        "      description='mask contrast',\n",
        "      disabled=False,\n",
        "      continuous_update=False,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='.2f',\n",
        "  )\n",
        "\n",
        "  display.display(wmask_contrast_adjust)\n",
        "\n",
        " \n",
        "\n",
        "  #####---wupscale----\n",
        "\n",
        "  from ipywidgets import interact\n",
        "  wupscale=widgets.ToggleButtons(\n",
        "    options=['0x', '1x', '2x', '4x'],\n",
        "    description='ERSGAN:',\n",
        "    disabled=False,\n",
        "    button_style='', # 'success', 'info', 'warning', 'danger' or '',\n",
        "    layout=widgets.Layout(width='250px'),\n",
        "    tooltips=['No upscale','Restoration without upscale', '2x', '4x'],\n",
        "\n",
        ")\n",
        "  \n",
        "  def wupscale_listener(sender):\n",
        "    if wupscale.value=='0x':\n",
        "      wfacerestore.value='Disabled'\n",
        "      wfacerestore.disabled=True\n",
        "    else:\n",
        "       wfacerestore.disabled=False\n",
        "    return\n",
        "\n",
        "  wupscale.observe(wupscale_listener, 'value')\n",
        "\n",
        "  ##@interact\n",
        "  #def clicked(a=wupscale()):\n",
        "  #  print(a)\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  #####---wfacerestore----\n",
        "\n",
        "  wfacerestore=widgets.ToggleButtons(\n",
        "    options=['Disabled', 'Face Restoration'],\n",
        "    description='',\n",
        "    disabled=True,\n",
        "    button_style='', # 'success', 'info', 'warning', 'danger' or '',\n",
        "    layout=widgets.Layout(width='200px'),\n",
        "    tooltips=['No Restoration','Face restoration']\n",
        "\n",
        ")\n",
        "\n",
        "  wshowups=widgets.HBox([wupscale, wfacerestore])\n",
        "  display.display(wshowups)\n",
        "\n",
        "   #####---confgimg----\n",
        "\n",
        "  wconfgimg=widgets.Text(\n",
        "      value='',\n",
        "      placeholder='Path from Image Config',\n",
        "      description='Config Image',\n",
        "      disabled=False   \n",
        "  )\n",
        "\n",
        "  display.display(wconfgimg)\n",
        "\n",
        "  wfile=open(\"/content/drive/MyDrive/AI/auxiliar/empty.png\", \"rb\")\n",
        "  wimgempty= wfile.read()\n",
        "\n",
        "   #####---showinterimg----\n",
        "\n",
        "  wshowinterimg=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Show Sampling',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        "  display.display(wshowinterimg)\n",
        "\n",
        "  #####---saveinterimg----\n",
        "\n",
        "  wsaveinterimg=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Save Sampling',\n",
        "      disabled=False,\n",
        "      indent=True\n",
        "  )\n",
        "\n",
        " \n",
        "  display.display(wsaveinterimg)\n",
        "\n",
        "  #####---skipsamples----\n",
        "\n",
        "  wskipsamples=widgets.IntSlider(\n",
        "      value=10,\n",
        "      min=1,\n",
        "      max=25,\n",
        "      step=1,\n",
        "      description='Skip Samples',\n",
        "      disabled=False,\n",
        "      continuous_update=False,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='d'\n",
        "  )\n",
        "\n",
        "  display.display(wskipsamples)\n",
        "\n",
        "  def LoadImgConfig(vbutton):\n",
        "  #789\n",
        "   imgbytes=(wupload.value.get(wupload.metadata[0].get('name'))).get('content')\n",
        "   wimgshowconf.value=imgbytes\n",
        "   wimgresult.value=imgbytes\n",
        "   smetadata=Image.open(io.BytesIO(imgbytes)).info\n",
        "   if bool(smetadata):\n",
        "      MetaToParam(smetadata,parenv)\n",
        "   else:\n",
        "      print(\"Image without configuration parameters!\")\n",
        "  \n",
        "  \n",
        "\n",
        " #####---wupload, wloadmeta---\n",
        "\n",
        "  wupload=widgets.FileUpload(\n",
        "  accept='.png',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
        "  multiple=False  # True to accept multiple files upload else False \n",
        "  )\n",
        "\n",
        "  wupload.observe(LoadImgConfig, 'value')\n",
        "\n",
        "\n",
        "  tfile=open(\"/content/drive/MyDrive/AI/auxiliar/empty.png\", \"rb\")\n",
        "  timage=image = tfile.read()\n",
        "\n",
        "  wimgshowconf=widgets.Image(\n",
        "    value=timage,\n",
        "    format='png',\n",
        "    width=32,\n",
        "    height=32,\n",
        "  )\n",
        "\n",
        "  ui = widgets.HBox([wupload, wimgshowconf])  \n",
        "\n",
        "\n",
        "  display.display(ui)\n",
        "\n",
        "  def wstartevent(sender):\n",
        " \n",
        "    #sender.disabled=True\n",
        "    sender.description='Processing...'\n",
        "    sender.button_style='warning'\n",
        "    initinference(parenv)\n",
        "    #sender.disabled=False\n",
        "    sender.description='START!'\n",
        "    sender.button_style='info'\n",
        "\n",
        "#####---prompt----\n",
        "\n",
        "  wprompt=widgets.Textarea(\n",
        "      value='gorgeous woman with a long hair waving with wind, intricate, elegant, sharp focus, illustration, highly detailed, digital painting, concept art, matte, art by WLOP and Artgerm and Greg Rutkowski and Alphonse Mucha, masterpiece',\n",
        "      placeholder='Type something',\n",
        "      description='Prompt:',\n",
        "      disabled=False,\n",
        "      layout=widgets.Layout(height='200px'),\n",
        "  )\n",
        "  \n",
        "\n",
        "  #####---wnegativeprompt----\n",
        "\n",
        "  wnegativeprompt=widgets.Textarea(\n",
        "      value='',\n",
        "      placeholder='Type something',\n",
        "      description='Negative:',\n",
        "      disabled=False,\n",
        "      layout=widgets.Layout(height='200px'),\n",
        "  )\n",
        "   \n",
        "  uipn = widgets.HBox([wprompt, wnegativeprompt])\n",
        "  display.display(uipn)  \n",
        "\n",
        " #####---H----\n",
        "\n",
        "  wh=widgets.IntText(\n",
        "      value=512,\n",
        "      description='H',\n",
        "      disabled=False\n",
        "  )\n",
        "\n",
        "  #display.display(wh)\n",
        "\n",
        " \n",
        "  #####---W----\n",
        "\n",
        "  ww=widgets.IntText(\n",
        "      value=512,\n",
        "      description='W',\n",
        "      disabled=False\n",
        "  )\n",
        "\n",
        "  #display.display(ww)\n",
        "\n",
        "  uiwwwh = widgets.HBox([wh, ww])  \n",
        "  display.display(uiwwwh)\n",
        "\n",
        "  #####---n_batch----\n",
        "\n",
        "  wn_batch=widgets.IntText(\n",
        "      value=1,\n",
        "      description='n batch',\n",
        "      disabled=False\n",
        "  )\n",
        "\n",
        "  #display.display(wn_batch)  \n",
        "\n",
        "  #####---batch_name----\n",
        "\n",
        "  wbatch_name=widgets.Text(\n",
        "      value='',\n",
        "      placeholder='Name the Batch folder',\n",
        "      description='batch name',\n",
        "      disabled=False   \n",
        "  )\n",
        "\n",
        "  #display.display(wbatch_name)\n",
        "  ui_nbatch_batch_name = widgets.HBox([wn_batch, wbatch_name])  \n",
        "  display.display(ui_nbatch_batch_name)\n",
        "\n",
        "  #####---filename_format----\n",
        "\n",
        " #####---seed----\n",
        "\n",
        "  wseed=widgets.IntText(\n",
        "      value=-1,\n",
        "      description='seed',\n",
        "      disabled=False\n",
        "  )\n",
        "\n",
        "  #display.display(wseed)\n",
        "\n",
        "  #####---sampler----\n",
        "\n",
        "  wsampler=widgets.Dropdown(\n",
        "      options=[\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\",\"plms\", \"ddim\"],\n",
        "      value='klms',\n",
        "      description='Sampler',\n",
        "      disabled=False,\n",
        "  )\n",
        "\n",
        "\n",
        "  #display.display(wsampler)\n",
        "\n",
        "  ui_seed_sampler = widgets.HBox([wseed, wsampler])  \n",
        "  display.display(ui_seed_sampler)  \n",
        "\n",
        "#####---steps----\n",
        "\n",
        "  def wsteps_listener(sender):\n",
        "    wimgtrans.value=1\n",
        "    wimgtrans.max=int(wsteps.value*(1-wstrength.value))\n",
        "    \n",
        "    \n",
        "\n",
        "  wsteps=widgets.IntSlider(\n",
        "      value=50,\n",
        "      min=0,\n",
        "      max=400,\n",
        "      step=5,\n",
        "      description='Steps',\n",
        "      disabled=False,\n",
        "      continuous_update=False,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='d'\n",
        "  )\n",
        "\n",
        "  wsteps.observe(wsteps_listener, 'value')\n",
        "\n",
        "  #display.display(wsteps)\n",
        "\n",
        "     #####---scale----\n",
        "\n",
        "  wscale=widgets.FloatSlider(\n",
        "      value=7.5,\n",
        "      min=0,\n",
        "      max=50.0,\n",
        "      step=0.5,\n",
        "      description='Scale',\n",
        "      disabled=False,\n",
        "      continuous_update=False,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='.1f',\n",
        "  )\n",
        "\n",
        "  #display.display(wscale)\n",
        "\n",
        "  ui_steeps_scale = widgets.HBox([wsteps, wscale])  \n",
        "  display.display(ui_steeps_scale)  \n",
        "\n",
        "#####---strength----\n",
        "\n",
        "  wstrength=widgets.FloatSlider(\n",
        "      value=0,\n",
        "      min=0,\n",
        "      max=1,\n",
        "      step=0.05,\n",
        "      description='strength',\n",
        "      disabled=False,\n",
        "      continuous_update=False,\n",
        "      orientation='horizontal',\n",
        "      readout=True,\n",
        "      readout_format='.2f',\n",
        "  )\n",
        "\n",
        "  wstrength.observe(wsteps_listener, 'value')\n",
        "  display.display(wstrength)\n",
        "\n",
        "#####---wstart---\n",
        "  def showpaint(sender):\n",
        "    vcanvas=buildcanvas(512,512,10,'/content/temporal/1.png')\n",
        "    cpaint(vcanvas,'/content/temporal/mask.png')\n",
        "\n",
        "  wstart = widgets.Button(\n",
        "  description='START',\n",
        "  disabled=False,\n",
        "  button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
        "  tooltip='Click me',\n",
        "  icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "  )\n",
        "\n",
        "  wstart.on_click(wstartevent)\n",
        "  #display.display(wstart)\n",
        "\n",
        "  #####---wresulttoinit---\n",
        "\n",
        "  def ResultToInit(sender):\n",
        "    img = Image.open(io.BytesIO(wimgresult.value),mode='r')\n",
        "    filename=\"temporal.png\"\n",
        "    OriginFilePath=os.path.join(temporal_path,filename)\n",
        "    img.save(OriginFilePath, format='PNG')\n",
        "    img.close()\n",
        "    wuse_init.value=True\n",
        "    winit_image.value=OriginFilePath\n",
        "    sender.button_style='warning'\n",
        "    wretro.value=\"True\"\n",
        "\n",
        "\n",
        "  def ResultToInitoff(sender):\n",
        "    \n",
        "    wuse_init.value=False\n",
        "    winit_image.value=\"\"\n",
        "    wstrength.value=0\n",
        "    wresulttoinit.button_style='info'\n",
        "    wretro.value=\"False\"\n",
        "    \n",
        "\n",
        "  wresulttoinit = widgets.Button(\n",
        "  description='RETRO ON',\n",
        "  disabled=False,\n",
        "  button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
        "  tooltip='Click me',\n",
        "  icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "  )\n",
        "\n",
        "  wresulttoinit.on_click(ResultToInit)\n",
        "  #display.display(wresulttoinit)\n",
        "\n",
        "  wresulttoinitoff = widgets.Button(\n",
        "  description='RETRO OFF',\n",
        "  disabled=False,\n",
        "  button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
        "  tooltip='Click me',\n",
        "  icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "  )\n",
        "\n",
        "  wresulttoinitoff.on_click(ResultToInitoff)\n",
        " # display.display(wresulttoinitoff)\n",
        "\n",
        " #####---wshowpaint---\n",
        "\n",
        "  wshowpaint = widgets.Button(\n",
        "  description='MASK',\n",
        "  disabled=False,\n",
        "  button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
        "  tooltip='Image mask',\n",
        "  icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "  )\n",
        "\n",
        "  wshowpaint.on_click(showpaint)\n",
        "\n",
        "  ui_start_wresulttoinit_wresultoinitoff = widgets.HBox([wstart, wresulttoinit, wresulttoinitoff, wshowpaint])  \n",
        "  display.display(ui_start_wresulttoinit_wresultoinitoff) \n",
        "\n",
        "  #####---wimgtrans----\n",
        "\n",
        "  def wimgtrans_listener(sender):\n",
        "    internamefile=os.path.join(temporal_path, str(wimgtrans.value) + \".png\")\n",
        "    print(internamefile)   \n",
        "    img = open(internamefile,'rb')\n",
        "    wimgresult.value=img.read()\n",
        "   \n",
        "   \n",
        " \n",
        "\n",
        "\n",
        "  wimgtrans=widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=50,\n",
        "    step=1,\n",
        "    description='Intermedie:',\n",
        "    disabled=False,\n",
        "    continuous_update=False,\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='d'\n",
        ")\n",
        "  \n",
        "  wimgtrans.observe(wimgtrans_listener, 'value')\n",
        "  display.display(wimgtrans)\n",
        "\n",
        "\n",
        "  return locals()\n",
        "\n",
        "def BuildErsganParams(upsizeopt, faceopt, inputimage, outputpath):\n",
        "  \n",
        "  face=''\n",
        "  if faceopt:\n",
        "    face='--face_enhance'\n",
        "  origenimg=inputimage\n",
        "  targetpath=outputpath\n",
        "  \n",
        "  return (f'-n RealESRGAN_x4plus -i {origenimg} -o {targetpath} --outscale {upsizeopt}  {face} --suffix ersgan ')\n",
        "\n",
        "\n",
        "\n",
        "   # if it is out of memory, try to use the `--tile` option\n",
        "  # We upsample the image with the scale factor X3.5\n",
        "\n",
        "  # par='-n RealESRGAN_x4plus -i upload -o results --outscale 4 --fp32 --face_enhance'\n",
        "  # !python inference_realesrgan.py $par\n",
        "  # Arguments\n",
        "  # -n, --model_name: Model names\n",
        "  # -i, --input: input folder or image\n",
        "  # --outscale: Output scale, can be arbitrary scale factore. \n",
        "  #[-h] [-i INPUT] [-n MODEL_NAME] [-o OUTPUT]\n",
        "  #                               [-dn DENOISE_STRENGTH] [-s OUTSCALE]\n",
        "  #                               [--model_path MODEL_PATH] [--suffix SUFFIX]\n",
        "  #                               [-t TILE] [--tile_pad TILE_PAD]\n",
        "  #                              [--pre_pad PRE_PAD] [--face_enhance] [--fp32]\n",
        "  #                               [--alpha_upsampler ALPHA_UPSAMPLER] [--ext EXT]\n",
        "  #                              [-g GPU_ID]\n",
        "    \n",
        "\n",
        "\n",
        "def DeforumArgs(args):\n",
        "      \n",
        "\n",
        "      class counter:\n",
        "       \n",
        "        def __init__(self):\n",
        "          self.x=0\n",
        "        def inc(self):\n",
        "           self.x=self.x+1\n",
        "        def reset(self):\n",
        "           self.x=0\n",
        "        def value(self):\n",
        "           return self.x\n",
        "        \n",
        "\n",
        "      BatchPrompts=False #@param {type:\"boolean\"}\n",
        "\n",
        "      bprompts=[\"a beautiful forest by Asher Brown Durand, trending on Artstation\", #the first prompt I want\n",
        "            \"a beautiful portrait of a woman by Artgerm, trending on Artstation\", #the second prompt I want\n",
        "              #\"the third prompt I don't want it I commented it with an\",\n",
        "            ]\n",
        "\n",
        "      retro=cbool(args.wretro.value)\n",
        "      showinterimg=args.wshowinterimg.value\n",
        "      saveinterimg=args.wsaveinterimg.value\n",
        "      listinterimg=[]\n",
        "      skipsamples=args.wskipsamples.value\n",
        "      ##cuidado multiples referencias\n",
        "      countsamples=counter()\n",
        "      countsamples.reset()\n",
        "      incr_batch=False;\n",
        "      incr_every=0;\n",
        "      upscale_scale=args.wupscale.value[:1]\n",
        "      face_restore=False;\n",
        "      if args.wfacerestore.value!='Disabled':\n",
        "        face_restore=True;\n",
        "      prompt=args.wprompt.value\n",
        "      negativeprompt=args.wnegativeprompt.value\n",
        "      prompts=[]\n",
        "      if BatchPrompts==False:\n",
        "        prompts.append(prompt)\n",
        "      else:\n",
        "        prompts=bprompts\n",
        "\n",
        "      W = args.ww.value\n",
        "      H = args.wh.value\n",
        "      W, H = map(lambda x: x - x % 64, (W, H))  # resize to integer multiple of 64\n",
        "\n",
        "     \n",
        "      seed = args.wseed.value\n",
        "      sampler = parenv.wsampler.value \n",
        "      steps = args.wsteps.value\n",
        "      scale = args.wscale.value\n",
        "      ddim_eta = args.wddim_eta.value\n",
        "      dynamic_threshold = None\n",
        "      static_threshold = None   \n",
        "\n",
        "      save_samples = args.wsave_samples.value\n",
        "      save_settings = args.wsave_settings.value\n",
        "      display_samples = args.wdisplay_samples.value\n",
        "      Save_Conf_in_Image= True\n",
        "\n",
        "     \n",
        "      n_batch = args.wn_batch.value\n",
        "      batch_name = args.wbatch_name.value\n",
        "      filename_format =args.wfilename_format.value\n",
        "      if incr_batch:\n",
        "        seed_behavior='fixed'\n",
        "      else: \n",
        "        seed_behavior = args.wseed_behavior.value\n",
        "      make_grid = args.wmake_grid.value\n",
        "      grid_rows = args.wgrid_rows.value\n",
        "      outdir = get_output_folder(output_path, batch_name)\n",
        "      \n",
        "     \n",
        "      use_init = args.wuse_init.value\n",
        "      strength = args.wstrength.value\n",
        "      init_image = args.winit_image.value\n",
        "      # Whiter areas of the mask are areas that change more\n",
        "      use_mask = args.wuse_mask.value\n",
        "      mask_file = args.wmask_file.value\n",
        "      invert_mask = args.winvert_mask.value\n",
        "      # Adjust mask image, 1.0 is no adjustment. Should be positive numbers.\n",
        "      mask_brightness_adjust = args.wmask_brightness_adjust.value\n",
        "      mask_contrast_adjust = args.wmask_contrast_adjust.value\n",
        "\n",
        "      n_samples = 1 # doesnt do anything\n",
        "      precision = 'autocast' \n",
        "      C = 4\n",
        "      f = 8\n",
        "\n",
        "      prompt = \"\"\n",
        "      timestring = \"\"\n",
        "      init_latent = None\n",
        "      init_sample = None\n",
        "      init_c = None\n",
        "\n",
        "      return locals()\n",
        "\n",
        "\n",
        "\n",
        "def next_seed(args):\n",
        "      if args.seed_behavior == 'iter':\n",
        "          args.seed += 1\n",
        "      elif args.seed_behavior == 'fixed':\n",
        "          pass # always keep seed the same\n",
        "      else:\n",
        "          args.seed = random.randint(0, 2**32)\n",
        "      return args.seed\n",
        "\n",
        "def render_image_batch(args, params):\n",
        "     \n",
        "      args.prompts = {k: f\"{v:05d}\" for v, k in enumerate(args.prompts)}\n",
        "      \n",
        "      # create output folder for the batch\n",
        "      os.makedirs(args.outdir, exist_ok=True)\n",
        "      if args.save_settings or args.save_samples:\n",
        "          print(f\"Saving to {os.path.join(args.outdir, args.timestring)}_*\")\n",
        "\n",
        "      # save settings for the batch\n",
        "      if args.save_settings:\n",
        "          filename = os.path.join(args.outdir, f\"{args.timestring}_settings.txt\")\n",
        "          with open(filename, \"w+\", encoding=\"utf-8\") as f:\n",
        "              json.dump(dict(args.__dict__), f, ensure_ascii=False, indent=4)\n",
        "\n",
        "      index = 0\n",
        "      \n",
        "      # function for init image batching\n",
        "      init_array = []\n",
        "      if args.use_init:\n",
        "          if args.init_image == \"\":\n",
        "              raise FileNotFoundError(\"No path was given for init_image\")\n",
        "          if args.init_image.startswith('http://') or args.init_image.startswith('https://'):\n",
        "              init_array.append(args.init_image)\n",
        "          elif not os.path.isfile(args.init_image):\n",
        "              if args.init_image[-1] != \"/\": # avoids path error by adding / to end if not there\n",
        "                  args.init_image += \"/\" \n",
        "              for image in sorted(os.listdir(args.init_image)): # iterates dir and appends images to init_array\n",
        "                  if image.split(\".\")[-1] in (\"png\", \"jpg\", \"jpeg\"):\n",
        "                      init_array.append(args.init_image + image)\n",
        "          else:\n",
        "              init_array.append(args.init_image)\n",
        "      else:\n",
        "          init_array = [\"\"]\n",
        "\n",
        "      # when doing large batches don't flood browser with images\n",
        "      clear_between_batches = args.n_batch >= 32\n",
        "\n",
        "      for iprompt, prompt in enumerate(args.prompts):  \n",
        "          args.prompt = prompt\n",
        "          print(f\"Prompt {iprompt+1} of {len(args.prompts)}\")\n",
        "          print(f\"{args.prompt}\")\n",
        "        \n",
        "          all_images = []\n",
        "\n",
        "          for batch_index in range(args.n_batch):\n",
        "              # if clear_between_batches and batch_index % 32 == 0: \n",
        "              #     display.clear_output(wait=True)            \n",
        "              print(f\"Batch {batch_index+1} of {args.n_batch}\")\n",
        "              \n",
        "              for image in init_array: # iterates the init images\n",
        "                  args.init_image = image\n",
        "                  print(args.steps)\n",
        "                  results = generate(args)\n",
        "                  for image in results:\n",
        "                      if args.make_grid:\n",
        "                          all_images.append(T.functional.pil_to_tensor(image))\n",
        "                      if args.save_samples:\n",
        "                          if args.filename_format == \"{timestring}_{index}_{prompt}.png\":\n",
        "                              partialfilename = f\"{args.timestring}_{index:05}_{sanitize(prompt)[:160]}\"\n",
        "                          elif args.filename_format == \"{timestring}_{index}_{steps}_{seed}.png\":\n",
        "                              partialfilename = f\"{args.timestring}_{index:05}_{args.steps}_{args.seed}\"\n",
        "                          else:\n",
        "                              partialfilename = f\"{args.timestring}_{index:05}_{args.seed}\"\n",
        "                          filename=partialfilename + \".png\"\n",
        "                          image.save(os.path.join(args.outdir,filename))\n",
        "                          if args.Save_Conf_in_Image:\n",
        "                            LastFile=ParamToMeta(partialfilename, args)\n",
        "                            if args.upscale_scale!='0':\n",
        "                              print(args.face_restore)\n",
        "                              bes= BuildErsganParams(args.upscale_scale, args.face_restore, LastFile, args.outdir)\n",
        "                              print(bes)\n",
        "                              !python inference_realesrgan.py $bes\n",
        "                              imgupsname=partialfilename + '_meta_ersgan.png'\n",
        "                              img2=open(os.path.join(args.outdir,imgupsname),'rb')\n",
        "                              wimgresultretouch.value=img2.read()\n",
        "                              img2.close\n",
        "\n",
        "                      if args.display_samples:\n",
        "                          buf = io.BytesIO()\n",
        "                          image.save(buf, format='PNG')\n",
        "                          byte_im = buf.getvalue()\n",
        "                          wimgresult.value=byte_im\n",
        "                          buf.flush\n",
        "                          buf.close\n",
        "                          image.close\n",
        "                          #789\n",
        "                      vlen=len(args.listinterimg)\n",
        "                      print(vlen)\n",
        "                      if vlen>0:\n",
        "                        print(args.retro)\n",
        "                        if args.retro==False:\n",
        "                          pathdiff=os.path.join(args.outdir,partialfilename) \n",
        "                          os.makedirs(pathdiff, exist_ok=True)\n",
        "                          for x in range(vlen):\n",
        "                            #print((vlen-1)-x)\n",
        "                            partialdiffpath=os.path.join(pathdiff, partialfilename +  \"_diff_\" + str((vlen-1)-x))\n",
        "                            #print(partialdiffpath)\n",
        "                            args.listinterimg[(vlen-1)-x].save(partialdiffpath + \".png\")\n",
        "                            if args.Save_Conf_in_Image:\n",
        "                              ParamToMeta(partialdiffpath, args)\n",
        "                            args.listinterimg[(vlen-1)-x].close \n",
        "                            del args.listinterimg[((vlen-1)-x)] \n",
        "                        else:\n",
        "                          for x in range(vlen):\n",
        "                            #000\n",
        "                            internamefile=os.path.join(temporal_path, str(((vlen-1)-x)+1) + \".png\")   \n",
        "                            args.listinterimg[(vlen-1)-x].save(internamefile)\n",
        "                            args.listinterimg[(vlen-1)-x].close \n",
        "                            del args.listinterimg[((vlen-1)-x)] \n",
        "\n",
        "\n",
        "                      index += 1\n",
        "                  args.seed = next_seed(args)\n",
        "                  if args.incr_batch:\n",
        "                    args.steps+=args.incr_every\n",
        "\n",
        "          #print(len(all_images))\n",
        "          if args.make_grid:\n",
        "              grid = make_grid(all_images, nrow=int(len(all_images)/args.grid_rows))\n",
        "              grid = rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "              filename = f\"{args.timestring}_{iprompt:05d}_grid_{args.seed}.png\"\n",
        "              grid_image = Image.fromarray(grid.astype(np.uint8))\n",
        "              grid_image.save(os.path.join(args.outdir, filename))\n",
        "              #display.clear_output(wait=True)            \n",
        "              #display.display(grid_image)\n",
        "              buf = io.BytesIO()\n",
        "              grid_image.save(buf, format='PNG')\n",
        "              byte_im = buf.getvalue()\n",
        "              wimgresult.value=byte_im\n",
        "\n",
        "\n",
        "\n",
        "def initinference (params):\n",
        " \n",
        "  args = SimpleNamespace(**DeforumArgs(params))\n",
        "  args.timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  args.strength = max(0.0, min(1.0, args.strength))\n",
        "\n",
        "  if args.seed == -1:\n",
        "      args.seed = random.randint(0, 2**32)\n",
        "  if not args.use_init:\n",
        "      args.init_image = None\n",
        "  if args.sampler == 'plms' and (args.use_init):\n",
        "      print(f\"Init images are n't supported with PLMS yet, switching to KLMS\")\n",
        "      args.sampler = 'klms'\n",
        "  if args.sampler != 'ddim':\n",
        "      args.ddim_eta = 0\n",
        "      \n",
        "  # dispatch to appropriate renderer\n",
        "\n",
        "  wimgresult.value=parenv.wimgempty\n",
        "  # print(\"!!!!!!!\")\n",
        "  # print(len(args.listinterimg))\n",
        "  # print(\"!!!!!!!\")\n",
        "  image=render_image_batch(args,params) \n",
        "  ##ERSGAN post processing\n",
        "\n",
        "  \n",
        "  # REparams=BuildErsganParams()\n",
        "  # par='-n RealESRGAN_x4plus -i upload -o results --outscale 4 --fp32 --face_enhance'\n",
        "  # # !python inference_realesrgan.py $par\n",
        " \n",
        "\n",
        "  \n",
        " \n",
        "      \n"
      ],
      "metadata": {
        "id": "uIt2mmMu0uJv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8RAo2zI-vQm"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "importlib.reload(OmegaConf)\n",
        "\n",
        "#from ipywidgets.widgets.widget_upload import ButtonStyle\n",
        "\n",
        "\n",
        "#@markdown **Param Builder**\n",
        "import base64\n",
        "import io\n",
        "parenv=SimpleNamespace(**InitParams())\n",
        "\n",
        "\n",
        "#####---imgresult----\n",
        "\n",
        "global wimgresult\n",
        "global wimgresultretouch\n",
        "\n",
        "#def drawimgresult(sender):\n",
        "\n",
        "wimgresult=widgets.Image(\n",
        "value=parenv.wimgempty,\n",
        "format='png',\n",
        "width='auto',\n",
        "height='auto'\n",
        ")\n",
        "\n",
        "wimgresultretouch=widgets.Image(\n",
        "value=parenv.wimgempty,\n",
        "format='png',\n",
        "width=str(parenv.ww.value),\n",
        "height=str(parenv.wh.value)\n",
        ")\n",
        "\n",
        "wimgresultbox=widgets.HBox([wimgresult, wimgresultretouch])\n",
        "\n",
        "display.display(wimgresultbox)\n",
        "#888\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YgVeGnkZMsBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown **Purge RAM**\n",
        "Enable_Clean=False #@param {type:\"boolean\"}\n",
        "if Enable_Clean: \n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "u_o7wa5_wYLJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Texto de tÃ­tulo predeterminado\n",
        "\n",
        "testfile=os.path.join(temporal_path,\"1.png\") \n",
        "maskout=os.path.join(temporal_path,\"mask.png\")\n",
        "img64=pngtobase64(testfile)\n",
        "cpaint(maskout,512,512,10,img64)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nklw5mlqq3bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Texto de tÃ­tulo predeterminado\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, Button, Box\n",
        "from IPython.display import display, HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "js_code = '''\n",
        "\n",
        "function draw() {\n",
        "  const ctx = document.getElementById('canvas').getContext('2d');\n",
        "  const img = new Image();\n",
        "  img.onload = () => {\n",
        "    ctx.drawImage(img, 0, 0);\n",
        "    ctx.beginPath();\n",
        "    ctx.moveTo(30, 96);\n",
        "    ctx.lineTo(70, 66);\n",
        "    ctx.lineTo(103, 76);\n",
        "    ctx.lineTo(170, 15);\n",
        "    ctx.stroke();\n",
        "  };\n",
        "  img.src = 'backdrop.png';\n",
        "}\n",
        "\n",
        "draw()\n",
        "'''\n",
        "js_code % ()\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ts7PAxfZddd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font color='red'>Drawing APP</font> {vertical-output: true, run: \"auto\"}\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, Button, Box\n",
        "from IPython.display import display, HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "\n",
        "\n",
        "Square_Size = 256 #@param [\"256\", \"256\"] {type:\"raw\"}\n",
        "Brush_Size = 50 #@param {type:\"slider\", min:0, max:100, step:5}\n",
        "filename = \"/content/temporal/imagen.png\"#@param [] {allow-input: true}\n",
        "\n",
        "js_code = '''\n",
        "<style>\n",
        "  .colors-buttons div {\n",
        "      width: 30px;\n",
        "      height: 30px;\n",
        "      margin: 2px;}\n",
        "  div {\n",
        "      display: flex;\n",
        "  }\n",
        "  canvas{border:1px solid black !important;}\n",
        "</style>\n",
        "<canvas id=\"myCanvas\" width=\"%d\" height=\"%d\"></canvas>\n",
        "<div class=\"colors-buttons\">\n",
        "  <div class=\"color\" style=\"background-color: #000000;\" id-color=\"#000000\"></div>\n",
        "  <div class=\"color\" style=\"background-color: #FFFFFF;\" id-color=\"#FFFFFF\"></div>\n",
        "  <div class=\"color\" style=\"background-color: #FFFF00;\" id-color=\"#FFFF00\"></div>\n",
        "  <div class=\"color\" style=\"background-color: #FF00FF;\" id-color=\"#FF00FF\"></div>\n",
        "  <div class=\"color\" style=\"background-color: #00FFFF;\" id-color=\"#00FFFF\"></div>\n",
        "  <div class=\"color\" style=\"background-color: #FF0000;\" id-color=\"#FF0000\"></div>\n",
        "  <div class=\"color\" style=\"background-color: #0000FF;\" id-color=\"#0000FF\"></div>\n",
        "  <div class=\"color\" style=\"background-color: #00FF00;\" id-color=\"#00FF00\"></div>\n",
        "</div>\n",
        "<script>\n",
        "  \n",
        "  var canvas = document.querySelector('canvas')\n",
        "  var ctx = canvas.getContext('2d')\n",
        "  ctx.fillStyle = 'white';\n",
        "  ctx.fillRect( 0, 0, canvas.width, canvas.height)\n",
        "  var Brush_Size = %d\n",
        "  //var img = new Image();\n",
        "  //img.onload = () => {\n",
        "  //ctx.drawImage(img, 0, 0);\n",
        "  //}\n",
        "  //img.src='https://iili.io/QBYTE7.png';\n",
        "  var button = document.querySelector('button')\n",
        "  var mouse = {x: 0, y: 0}\n",
        "  canvas.addEventListener('mousemove', function(e) {\n",
        "    mouse.x = e.pageX - this.offsetLeft\n",
        "    mouse.y = e.pageY - this.offsetTop\n",
        "  })\n",
        "  canvas.onmousedown = ()=>{\n",
        "    ctx.beginPath()\n",
        "    ctx.moveTo(mouse.x, mouse.y)\n",
        "    \n",
        "    canvas.addEventListener('mousemove', onPaint)\n",
        "  }\n",
        "  canvas.onmouseup = ()=>{\n",
        "    canvas.removeEventListener('mousemove', onPaint)\n",
        "  }\n",
        "  var onPaint = ()=>{\n",
        "    ctx.fillRect(mouse.x-( Brush_Size/2), mouse.y-(Brush_Size/2), Brush_Size, Brush_Size)\n",
        "    ctx.stroke()\n",
        "  }\n",
        "  const colors = document.getElementsByClassName('color');\n",
        " \n",
        "  Array.from(colors).forEach(color => {\n",
        "      color.addEventListener('click', (event) => {\n",
        "          const colorSelected = event.target.getAttribute('id-color');\n",
        "          ctx.fillStyle = colorSelected;\n",
        "      });\n",
        "  });\n",
        "    // FINISH BUTTON\n",
        "  var data = new Promise(resolve=>{\n",
        "    button.onclick = ()=>{\n",
        "      resolve(canvas.toDataURL('image/jpg'))\n",
        "    }\n",
        "  })\n",
        "</script>\n",
        "'''\n",
        "\n",
        "\n",
        "## Function to Appear Image Canvas\n",
        "def draw(filename=filename,  w=Square_Size, h=Square_Size, Brush_Size=Brush_Size):\n",
        "\n",
        "  display(HTML(js_code % (w, h, Brush_Size)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  if AttributeError:\n",
        "    pass\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return len(binary)\n",
        "  if button2.on_click(on_button_clicked2):\n",
        "    pass\n",
        "  #im = im.resize((width // 2, height // 2), resample=Image.ANTIALIAS)\n",
        "\n",
        "## Action for Reset Button\n",
        "def on_button_clicked(b):\n",
        " \n",
        "  with output:\n",
        "   \n",
        "    #display(HTML(canvas_html % ( w=$Square_Size, h=$Square_Size, Brush_Size=$Brush_Size)))\n",
        "    data = eval_js(\"data\")\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "      f.write(binary)\n",
        "  return len(binary)\n",
        "  \n",
        "\n",
        "## Show Save Button & Save outputs\n",
        "button = widgets.Button(description=\"Save\")\n",
        "button.on_click(on_button_clicked)\n",
        "output = widgets.Output()\n",
        "display(button, output)\n",
        "\n",
        "## Show Canvas for the First Time\n",
        "draw(filename=filename,  w=Square_Size, h=Square_Size, Brush_Size=Brush_Size)\n",
        "print(\"Image Saved at\")"
      ],
      "metadata": {
        "id": "c4EMwUen5DeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Texto de tÃ­tulo predeterminado\n",
        "from IPython.display import HTML, display\n",
        "from google.colab import output\n",
        "\n",
        "# Render the HTML.\n",
        "\n",
        "\n",
        "html_text = output.eval_js('document.body.innerText')\n",
        "\n",
        "# %%capture\n",
        "# print('hola')"
      ],
      "metadata": {
        "id": "zBX9mx9OD6FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Texto de tÃ­tulo predeterminado\n",
        "\n",
        "\n",
        "\n",
        "#key\n",
        "#Jr8YHwsSBgwOxm3WrzB1Wv1iiedv2P89RN6GSANqwiw=\n",
        "#b'&\\xbf\\x18\\x1f\\x0b\\x12\\x06\\x0c\\x0e\\xc6m\\xd6\\xaf0uZ\\xfdb\\x89\\xe7o\\xd8\\xff=D\\xde\\x86H\\x03j\\xc2,'\n",
        "#iv\n",
        "#rwAKlvWf3gK4IIh38oOZng==\n",
        "#b'\\xaf\\x00\\n\\x96\\xf5\\x9f\\xde\\x02\\xb8 \\x88w\\xf2\\x83\\x99\\x9e'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "style=\"border:1px solid #000000\"\n",
        "canvas_html = \"\"\"\n",
        "  <canvas width=%d height=%d\"></canvas>\n",
        "  <button>Finish</button>\n",
        "  <script>\n",
        "  var canvas = document.querySelector('canvas');\n",
        "  var ctx = canvas.getContext('2d');\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        " \n",
        "  let newImage = new Image();\n",
        "  newImage.src = 'file:///content/drive/MyDrive/AI/StableDiffusion/refs/Ref99.png';\n",
        "\n",
        "\n",
        "  newImage.onload = () => {\n",
        "    \n",
        "  ctx.drawImage(newImage, 0, 0, 256, 256);\n",
        "}\n",
        "\n",
        " \n",
        "  ctx.lineWidth = %d\n",
        "  ctx.fillStyle = \"red\"\n",
        "  var button = document.querySelector('button')\n",
        "  var mouse = {x: 0, y: 0}\n",
        "  canvas.addEventListener('mousemove', function(e) {\n",
        "    mouse.x = e.pageX - this.offsetLeft\n",
        "    mouse.y = e.pageY - this.offsetTop\n",
        "  })\n",
        "  canvas.onmousedown = ()=>{\n",
        "    ctx.beginPath()\n",
        "    ctx.moveTo(mouse.x, mouse.y)\n",
        "    canvas.addEventListener('mousemove', onPaint)\n",
        "  }\n",
        "  canvas.onmouseup = ()=>{\n",
        "    canvas.removeEventListener('mousemove', onPaint)\n",
        "  }\n",
        "  var onPaint = ()=>{\n",
        "    ctx.lineTo(mouse.x, mouse.y)\n",
        "    ctx.stroke()\n",
        "  }\n",
        "  var data = new Promise(resolve=>{\n",
        "    button.onclick = ()=>{\n",
        "      resolve(canvas.toDataURL('image/png'))\n",
        "    }\n",
        "  })\n",
        "  </script>\n",
        "  \"\"\"\n",
        "\n",
        "def draw(filename='drawing.png', w=256, h=256, line_width=4):\n",
        "    display.display(HTML(canvas_html % (w, h, line_width)))\n",
        "    data = eval_js(\"data\")\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "      f.write(binary)\n",
        "    return len(binary)\n",
        "\n",
        "draw()  \n",
        "\n",
        "\n",
        "####################\n",
        "\n",
        "#style=\"border:1px solid #000000\n",
        "# canvas_html = \"\"\"\n",
        "#   <canvas width='256' height='256' style=\"border:1px solid #000000\"></canvas>\n",
        "#   <button>Finish</button>\n",
        "#   <script>\n",
        "#   var canvas = document.querySelector('canvas');\n",
        "#   var ctx = canvas.getContext('2d');\n",
        " \n",
        "#   var id = ctx.createImageData(1,1); // only do this once per page\n",
        "#   var d  = id.data;                        // only do this once per page\n",
        "#   d[0]   = #FF;\n",
        "#   d[1]   = #FF;\n",
        "#   d[2]   = 0;\n",
        "#   d[3]   = 0;\n",
        "#   ctx.putImageData( id, 1, 1 );  \n",
        "\n",
        "#   ctx.fillStyle = \"#000000\";\n",
        "#   ctx.fillRect(20, 20, 1, 1);\n",
        "  \n",
        "#   </script>\n",
        "#   \"\"\"\n",
        "\n",
        "\n",
        "# display.display(HTML(canvas_html))\n",
        "   \n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# var myImg = new Image();\n",
        "# img.onload = function() {\n",
        "#    context.drawImage(myImg, 0, 0);\n",
        "# };\n",
        "# img.src = 'https://www.tutorialspoint.com/images/seaborn-4.jpg?v=2';\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   # if it is out of memory, try to use the `--tile` option\n",
        "#   # We upsample the image with the scale factor X3.5\n",
        "\n",
        "#   par='-n RealESRGAN_x4plus -i upload -o results --outscale 4 --fp32 --face_enhance'\n",
        "#   !python inference_realesrgan.py $par\n",
        "  # Arguments\n",
        "  # -n, --model_name: Model names\n",
        "  # -i, --input: input folder or image\n",
        "  # --outscale: Output scale, can be arbitrary scale factore. \n",
        "  #[-h] [-i INPUT] [-n MODEL_NAME] [-o OUTPUT]\n",
        "  #                               [-dn DENOISE_STRENGTH] [-s OUTSCALE]\n",
        "  #                               [--model_path MODEL_PATH] [--suffix SUFFIX]\n",
        "  #                               [-t TILE] [--tile_pad TILE_PAD]\n",
        "  #                              [--pre_pad PRE_PAD] [--face_enhance] [--fp32]\n",
        "  #                               [--alpha_upsampler ALPHA_UPSAMPLER] [--ext EXT]\n",
        "  #                              [-g GPU_ID]"
      ],
      "metadata": {
        "id": "udjJ3XxmHfo1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Texto de tÃ­tulo predeterminado\n",
        "from IPython.display import HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "canvas_html = \"\"\"\n",
        "<canvas width=%d height=%d style=\"border: 1px solid black;\"></canvas><br><br>\n",
        "<button id='clearButton'>Clear</button><br><br>\n",
        "<button id='circleButton'>Circle</button><br><br>\n",
        "<button id='drawButton'>Draw</button><br><br>\n",
        "<button id='creatureButton'>Place Creature</button><br><br>\n",
        "<label for=\"radius\">Radius of circle:</label><br><br>\n",
        "<input type=\"number\" value=20 id='radius'><br><br>\n",
        "<button id='finishButton'>Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "let newImage = new Image();\n",
        "newImage.onload = () => {\n",
        "ctx.drawImage(newImage, 0, 0, 256, 256);\n",
        "}\n",
        "newImage.src = 'https://iili.io/QBYTE7.png';\n",
        "   \n",
        "\n",
        "ctx.lineWidth = %d\n",
        "var button = document.getElementById('finishButton')\n",
        "var buttonCircle= document.getElementById('circleButton')\n",
        "var buttonDraw= document.getElementById('drawButton')\n",
        "var buttonClear= document.getElementById('clearButton')\n",
        "var buttonCreature= document.getElementById('creatureButton')\n",
        "var mouse = {x: 0, y: 0}\n",
        "var creature_location={x:0,y:0}\n",
        "var selected='draw'\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.addEventListener('mousedown', function(e) {\n",
        "    if(selected=='draw'){\n",
        "    ctx.beginPath()\n",
        "    ctx.moveTo(mouse.x, mouse.y)\n",
        "    canvas.addEventListener('mousemove', onPaint)\n",
        "    }\n",
        "    if(selected=='circle'){\n",
        "    mouse.x = e.pageX - this.offsetLeft\n",
        "    mouse.y = e.pageY - this.offsetTop\n",
        "    ctx.beginPath()\n",
        "    ctx.arc(mouse.x, mouse.y, document.getElementById('radius').value, 0, 2 * Math.PI)\n",
        "    ctx.fill()\n",
        "    }\n",
        "    if(selected=='place_creature'){\n",
        "      ctx.clearRect(creature_location.x-20, creature_location.y-20, 40, 40)\n",
        "      mouse.x = e.pageX - this.offsetLeft\n",
        "      mouse.y = e.pageY - this.offsetTop\n",
        "      ctx.fillStyle = \"red\";\n",
        "      ctx.beginPath();\n",
        "      ctx.rect(mouse.x-20, mouse.y-20, 40, 40);\n",
        "      creature_location.x=mouse.x\n",
        "      creature_location.y=mouse.y\n",
        "      ctx.fill();\n",
        "      ctx.fillStyle = \"black\";\n",
        "    }\n",
        "  \n",
        "})\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.arc(mouse.x, mouse.y, document.getElementById('radius').value, 0, 2 * Math.PI)\n",
        "  ctx.fill()\n",
        "  ctx.beginPath()\n",
        "}\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "  buttonCircle.onclick = ()=>{\n",
        "    selected='circle'\n",
        "  }\n",
        "  buttonDraw.onclick = ()=>{\n",
        "    selected='draw'\n",
        "  }\n",
        "  buttonClear.onclick= ()=>{\n",
        "    ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "  }\n",
        "  buttonCreature.onclick= ()=>{\n",
        "    selected='place_creature'\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def draw(filename='drawing.png', w=256, h=256, line_width=5):\n",
        "  a=HTML(canvas_html % (w, h, line_width))\n",
        "  display(a)\n",
        "  \n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  \n",
        "    \n",
        "  return len(binary)\n",
        "draw()"
      ],
      "metadata": {
        "id": "UCaGgHZr7CpM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font color='red'>DRAW APP V2</font> {vertical-output: true, run: \"auto\"}\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from datetime import datetime\n",
        "from termcolor import colored\n",
        "\n",
        "Square_Size = 256 #@param [\"256\", \"512\"] {type:\"raw\"}\n",
        "Brush_Size = 60 #@param {type:\"slider\", min:0, max:100, step:5}\n",
        "filename = \"your_name_here.jpg\"\n",
        "\n",
        "print(colored(\"Draw the boundary\",\"red\"))\n",
        "\n",
        "canvas_html = \"\"\"\n",
        "  <style>\n",
        "    .colors-buttons div {\n",
        "        width: 30px;\n",
        "        height: 30px;\n",
        "        margin: 1px;\n",
        "        border: 1px solid black !important;}\n",
        "    div {\n",
        "        display: inline-block;\n",
        "    }\n",
        "    canvas{border:1px solid black !important;}\n",
        "  </style>\n",
        "\n",
        "  <canvas id=\"myCanvas1\" width=\"%d\" height=\"%d\"></canvas>\n",
        " \n",
        "\n",
        "  <div class=\"colors-buttons\">\n",
        "    <div class=\"color\" style=\"background-color: #000000;\" id-color=\"#000000\"></div>\n",
        "    <div class=\"color\" style=\"background-color: #FFFFFF;\" id-color=\"#FFFFFF\"></div>\n",
        "    <div class=\"color\" style=\"background-color: #FFFF00;\" id-color=\"#FFFF00\"></div>\n",
        "    <div class=\"color\" style=\"background-color: #FF00FF;\" id-color=\"#FF00FF\"></div>\n",
        "    <div class=\"color\" style=\"background-color: #00FFFF;\" id-color=\"#00FFFF\"></div>\n",
        "    <div class=\"color\" style=\"background-color: #FF0000;\" id-color=\"#FF0000\"></div>\n",
        "    <div class=\"color\" style=\"background-color: #0000FF;\" id-color=\"#0000FF\"></div>\n",
        "    <div class=\"color\" style=\"background-color: #00FF00;\" id-color=\"#00FF00\"></div>\n",
        "  </div>\n",
        "\n",
        "  <div>\n",
        "    <button id=\"save\">Save</button>\n",
        "    <button id=\"reset\">Reset</button>\n",
        "    <button id=\"exit\">Exit</button>\n",
        "    <button id=\"load\">Load</button>\n",
        "  </div>\n",
        "  \n",
        "\n",
        "\n",
        "  <script>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  var canvas = document.getelementbyid('myCanvas')\n",
        "  var ctx = canvas.getContext('2d')\n",
        "\n",
        "  ctx.fillStyle = 'white';\n",
        "  ctx.fillRect( 0, 0, canvas.width, canvas.height)\n",
        "  var Brush_Size = %d\n",
        "  let newImage = new Image();\n",
        "  newImage.onload = () => {\n",
        "//ctx.drawImage(newImage, 0, 0, 16, 16);\n",
        "}\n",
        "newImage.src = 'https://iili.io/QBYTE7.png';\n",
        "\n",
        "\n",
        "  var button = document.querySelector('button')\n",
        "  var mouse = {x: 0, y: 0}\n",
        "  \n",
        "  var clear_button = document.querySelector('#reset')\n",
        "  var load=document.querySelector('#load')\n",
        "  var button = document.querySelector('#save')\n",
        "  var exit_button = document.querySelector('#exit')\n",
        "  \n",
        "  canvas.addEventListener('mousemove', function(e) {\n",
        "    mouse.x = e.pageX - this.offsetLeft\n",
        "    mouse.y = e.pageY - this.offsetTop\n",
        "  })\n",
        "  canvas.onmousedown = ()=>{\n",
        "    ctx.beginPath()\n",
        "    ctx.moveTo(mouse.x, mouse.y)\n",
        "    \n",
        "    canvas.addEventListener('mousemove', onPaint)\n",
        "  }\n",
        "  canvas.onmouseup = ()=>{\n",
        "    canvas.removeEventListener('mousemove', onPaint)\n",
        "  }\n",
        "  var onPaint = ()=>{\n",
        "\n",
        "    ctx.fillRect(mouse.x-( Brush_Size/2), mouse.y-(Brush_Size/2), Brush_Size, Brush_Size)\n",
        "    ctx.stroke()\n",
        "  }\n",
        "\n",
        "  const colors = document.getElementsByClassName('color');\n",
        "\n",
        "  Array.from(colors).forEach(color => {\n",
        "      color.addEventListener('click', (event) => {\n",
        "          const colorSelected = event.target.getAttribute('id-color');\n",
        "          ctx.fillStyle = colorSelected;\n",
        "      });\n",
        "  });\n",
        "\n",
        "   \n",
        "    clear_button.onclick = ()=>{{\n",
        "        console.log('Clearing Screen')\n",
        "        ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "        ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "      }}\n",
        "      canvas.addEventListener('load', function() {{\n",
        "      console.log('All assets are loaded')\n",
        "    }})\n",
        "    //var data = new Promise(resolve=>{{\n",
        "    //  button.onclick = ()=>{{\n",
        "     //   resolve(canvas.toDataURL('image/png'))\n",
        "    //  }}\n",
        "\n",
        " var data = new Promise(resolve=>{{\n",
        "      load.onclick = ()=>{{\n",
        "        resolve(newImage.toString())\n",
        "      }}\n",
        "\n",
        "      exit_button.onclick = ()=>{{\n",
        "      resolve()\n",
        "    }}\n",
        "      \n",
        "    }})\n",
        "        \n",
        "\n",
        "  </script>\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "## Function to Appear Image Canvas\n",
        "def draw(filename=filename,  w=Square_Size, h=Square_Size, Brush_Size=Brush_Size):\n",
        "  display(HTML(canvas_html % (w, h, Brush_Size)))\n",
        "  data = eval_js(\"data\")\n",
        "  print(data)\n",
        "  # binary = b64decode(data.split(',')[1])\n",
        "  # if AttributeError:\n",
        "  #   pass\n",
        "  # with open(filename, 'wb') as f:\n",
        "  #   f.write(binary)\n",
        "  # return len(binary)\n",
        "\n",
        "\n",
        "  \n",
        "## Action for Save Button\n",
        "def on_button_clicked(c):\n",
        "  #draw(filename=filename,  w=Square_Size, h=Square_Size, Brush_Size=Brush_Size)\n",
        "  with output:\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Image Saved Again at:\", current_time)\n",
        "  \n",
        "## Show Save Button & Save outputs\n",
        "button = widgets.Button(description=\"Save Image\")\n",
        "button.on_click(on_button_clicked)\n",
        "output = widgets.Output()\n",
        "display(button, output)\n",
        "\n",
        "## Show Canvas for the First Time\n",
        "draw(filename=filename,  w=Square_Size, h=Square_Size, Brush_Size=Brush_Size)"
      ],
      "metadata": {
        "id": "uHiguaIyxB-p",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Texto de tÃ­tulo predeterminado\n",
        "import base64\n"
      ],
      "metadata": {
        "id": "1M6StAir-YTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "img64=pngtobase64('/content/temporal/1.png')\n",
        "cpaint('/content/temporal/drawing.png',512,512,10,img64)"
      ],
      "metadata": {
        "id": "WOp-vEnBfbio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img64=pngtobase64(testfile)\n",
        "cpaint(maskout,512,512,10,img64)"
      ],
      "metadata": {
        "id": "B0Cs8RPxo_Jx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "c442uQJ_gUgy"
      ],
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}